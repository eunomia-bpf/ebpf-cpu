你说：
BEAR：BPF-化的跨频率+空闲+热管理一体化调速器把这个更详细地讲一下，搜索一下相关的工作（尤其是论文），然后给我一个完整的研究提案，讲一下这个啥做的，contri是啥，和其他对比，实验方案和工作负载是什么，有哪些平台可以考虑实验，挑战是啥，能有什么影响之类的。要严格按照论文和研究提案的形式。还可以再考虑和gpu此类地没有可能进一步关联。至少50个以上的引用

ChatGPT 说：
为了确保研究课题内容完整，我想确认几个细节：

BEAR调控器的具体目标是否是设计一个统一的调度策略来跨CPU频率调节、空闲状态管理和热管理？是否已经初步实现或者只是一个概念？

你希望这个调速器工作在哪些具体平台（例如移动SoC、数据中心服务器、嵌入式系统）？

是否希望这项研究主题也以 GPU 的频率和热管理协调，或者只关注 CPU？

有没有目标操作系统（如Linux内核）或目标调度框架（如EAS、schedutil）？

是否有偏好的引用来源（例如更偏学术，还是Linux内核/主线补丁讨论也可以）？

这些信息将帮助我为您生成一个响亮、有逻辑、有引用的研究提案。








你说：
1.只是一个概念，具体需要你帮我想一想，除了频率+休闲+热管理还可以有更多什么？核心的标题，故事和见解应该怎么写，有几种写法？2. 不确定，你想，例如移动SoC、数据中心服务器、嵌入式系统，还有没有？差异是什么？3. 也涉及 GPU 的频率和热管理协调 4. 目标 Linux 5. 主要学术，或者 Linux 内核/主线补丁讨论之类也可以。越详细越好。

ChatGPT 说：
好的，我将调研与BEAR调控器构想相关的已有研究和主线内核工作，尤其是涉及BPF驱动的跨CPU/GPU频率调节、空闲管理、热管理、能效优化等多融合策略方向。

我会重点参考：

学术论文（来自顶级系统会议如SOSP/OSDI/ASPLOS/ATC/USENIX Security等）

Linux内核主线讨论（LKML）、schedutil、Thermal、cpufreq、cpuidle、GPU Governors相关子系统工作

相关BPF/XDP/BTF动态治理和跟踪工具（如BPFTrace、PMU、热区传感器跟踪等）

研究提案将包括：

多种故事线/insight 切入角度

不同工作负载、平台（移动SoC、服务器、IoT/嵌入式、HPC等）的场景差异

与 GPU 管理和设计可能面临的良好挑战

实验平台、基准、测量指标、预期难点

与已有的 Governor 和 Scheduler 的对比

可引用的重要工作（目标至少 50 篇）

我整理好后会给你一个符合论文/研究提案结构的文档版本。


研究耗时 10 分钟
· 19 个来源
· 152 次搜索
支持 BPF 的统一调速器，用于跨频率、空闲、散热和 GPU 协调

BEAR：基于 BPF 的 Linux 统一 CPU 频率、空闲和热管理调控器
引言和动机
现代处理器采用多种电源管理机制，尤其是动态电压和频率调节 (DVFS)、空闲状态管理和热节流，以平衡性能和能耗。然而，这些机制传统上各自为政，通常无法感知彼此的决策。
lwn.net
androidauthority.com
例如，Linux cpufreq子系统根据负载独立调整 CPU 频率，cpuidle子系统决定何时以及深度让 CPU 进入睡眠状态，而热控制框架则会限制性能以防止过热
lwn.net
androidauthority.com
这些子系统之间缺乏集成可能会导致相互冲突或次优的决策：调度程序可能会唤醒空闲的核心以提高性能，而 cpuidle 则试图让核心保持休眠状态，或者 cpufreq 可能会提高 CPU 的频率，而没有意识到热调节器很快就会将其调低
androidauthority.com
androidauthority.com
。这种不相交的操作，每个组件都在不同的时间尺度上工作，往往会产生相互矛盾的目的
androidauthority.com
并且无法实现全局最优的功率性能权衡。 无论是从业者还是研究人员，都已经认识到需要一种统一的 CPU 电源管理方法。像 Ingo Molnar 这样的 Linux 内核开发人员指出，任何“功耗感知”的调度解决方案都必须考虑所有CPU 电源控制——频率、空闲和调度器决策——而不是孤立地处理它们。
lwn.net
事实上，Molnar 明确表示，在社区解决 cpufreq 和 cpuidle 与调度程序的集成问题之前，不会合并任何新的功耗感知调度程序。
lwn.net
后续的努力，例如节能调度 (EAS)，通过协调调度程序与 DVFS 和空闲状态，朝着这个方向迈出了一步。EAS 引入了平台的能耗模型，旨在将 CPU 频率调节 (DVFS)、CPU 空闲，甚至热管理（通过 ARM 的智能功率分配 (IPA)）集成到调度决策中。
androidauthority.com
这个想法是让调度程序能够预测在给定 CPU 上调度任务对能量的影响，并选择能够最小化所需性能能量的位置和频率
androidauthority.com
本质上，EAS 及其相关补丁开始将 CPU 集群视为一个整体，而不是将频率和空闲选择完全交给不同的调控器。例如，ARM 的 IPA 会在 CPU（大核和小核）和 GPU 之间动态分配共享的热预算，通过协调利用所有可用的热余量，实现高达36% 的性能提升。
androidauthority.com
这表明，统一管理CPU-GPU 频率和热限制可以通过避免保守节流来显著提高热约束下的性能
androidauthority.com
androidauthority.com
类似地，EAS 表明，将任务打包到更少的 CPU 上（让其他 CPU 空闲）可以节省能源，且性能损失几乎可以忽略不计，而早期的 Linux 行为会分散任务以最大化性能，但代价是功耗
androidauthority.com
。 尽管取得了这些进展，但 Linux 电源管理的现状仍然是多个调控器半独立地工作。默认的 DVFS 调控器（例如schedutil）会对 CPU 利用率做出反应，cpuidle 调控器（例如“菜单”或较新的TEO - 面向定时器事件的调控器）会预测空闲时长以选择 C ​​状态，而热框架会在温度超过阈值时应用紧急制动。它们仅以有限的方式进行通信（例如，热节流可以间接限制最大频率，并schedutil间接考虑任务调度），但没有一个策略实体能够共同决定频率、空闲和热响应。调度器仍然在很大程度上不知道 cpuidle/cpufreq 的决策，反之亦然。
lwn.net
这种碎片化的设计会导致效率低下：例如，cpuidle 调控器可能会让 CPU 处于浅空闲状态，等待即将被唤醒，而 cpufreq 则会根据观察到的空闲时间同时降低该 CPU 的频率——这是一个矛盾的决定。再比如，独立的 CPU 和 GPU DVFS 调控器可能各自在独立状态下都能达到最佳性能，但它们不协调的操作可能会导致级联性能问题或热振荡。
arxiv.org
最近的一项关于移动大型 LLM 推理的研究强调，当前的移动 CPU/GPU DVFS 调速器一起使用时远非最佳，并且存在许多频率组合，其延迟和能耗均优于默认调速器
arxiv.org
arxiv.org
先前的研究提出了针对给定工作负载寻找最佳 CPU-GPU 频率设置
arxiv.org
但这些研究都没有研究多个 DVFS 控制器（CPU、GPU、内存）之间复杂的运行时相互作用，也没有提供统一的控制策略
arxiv.org
所有这些都表明，更全面的调控可以带来效率和绩效的大幅提升。 另一个挑战是，Linux 中的“一刀切”调控器（例如ondemand、schedutil或menuidle调控器）针对一般用途进行了调整，可能并不适合特定的工作负载或平台
lwn.net
。例如，默认的空闲启发式方法可能会错误预测具有不规则突发的工作负载，从而导致空闲时间过浅（浪费电力）或空闲时间过深（损害延迟）。
lwn.net
lwn.net
同样，针对平均工作负载进行调优的频率调节器可能无法快速提升以适应延迟敏感的交互式任务，或者相反，对于稳定的计算密集型作业，频率调节器可能会出现过大的波动。事实上，cpuidle 的“菜单”调节器使用基于过去中断时间的启发式方法来猜测下一个空闲时长。
lwn.net
；如果工作负载模式发生变化，它很容易猜测错误并进入不适当的空闲状态
lwn.net
lwn.net
。由于难以在不同场景下调优这些调速器，平台供应商（例如 Android 设备制造商）常常不得不开发自己的自定义调速器或用户空间守护进程（例如，Android 的交互式调速器或特定于供应商的热驱动程序）。这是一种劳动密集型的方法，会导致解决方案碎片化，难以适应或向上游迁移。 综上所述，我们迫切需要一个集成且可扩展的电源管理调速器，该调速器应 (1) 在一个框架内协调 DVFS、空闲和热管理决策，以避免冲突并利用跨领域知识；(2) 能够轻松定制以适应特定的工作负载或硬件。我们提出了BEAR，这是一种基于 BPF 的节能自适应调节机制，本质上是一个统一的 CPU 频率、空闲状态和热管理调速器，使用 eBPF（扩展伯克利包过滤器）技术实现。BEAR 将在 Linux 内核中运行，以做出全面的电源管理决策，但与固定的内核算法不同，它将利用 eBPF 来实现安全且可在运行时自定义的策略。我们认为，Linux 调度程序和 BPF 基础设施的最新进展使得以可控的方式将自定义逻辑注入电源管理成为可能。
lwn.net
free5gc.org
通过使用 eBPF，BEAR 可以在内核上下文中以较低的开销运行，并使用用户定义的策略脚本响应事件（例如任务调度、CPU 空闲进入/退出或热传感器更新）。这种方法带来了类似于用户空间解决方案的可扩展性，但具有内核执行的效率。它反映了将 eBPF 应用于其他子系统的持续趋势（例如，Facebook/Metasched_ext允许开发人员通过 BPF 加载新的 CPU 调度程序）。
lwn.net
phoronix.com
) – 从而使开发人员能够进行创新和优化，而无需维护复杂的内核分支或树外模块
lwn.net
lwn.net
。 具体来说，BEAR 旨在统一 Morten Rasmussen 2013 年补丁系列中所谓的“动力驱动”方法
lwn.net
lwn.net
，但为了灵活性，通过 eBPF 实现。在这个愿景中，单个驱动程序将包含 cpufreq 和 cpuidle，并公开类似go_faster()/ 的钩子go_slower()来请求性能更改并enter_idle()控制空闲状态的进入。
lwn.net
lwn.net
我们的工作基于这一理念：使用 BPF 钩子，调度程序和电源管理逻辑可以进行双向通信。BEAR 的 BPF 程序可以接收调度程序信号（例如 CPU 负载、任务事件）和硬件反馈（温度、功率读数），然后决定：(a) 为每个集群设置合适的 CPU 频率（或 DVFS 状态）；(b) 是保持核心活跃还是让其进入更深的空闲状态；以及 (c) 采取任何主动措施来避免热紧急情况（例如限制频率或注入空闲周期以进行冷却）。BEAR 不会让各个启发式算法相互冲突，而是会同时考虑所有因素，选择最佳操作。例如，如果一个突发任务到达空闲系统，统一策略可能会预先提高频率，并让某些核心不进入深度睡眠状态，从而预测负载；而传统系统可能会无意中做出相反的操作（空闲状态持续深度导致唤醒延迟，然后 cpufreq 反应缓慢）。
lwn.net
相反，在持续中等负载下，BEAR 可能会决定以稍低的频率运行任务，但避免唤醒额外的核心，从而节省能源，并将性能影响降至最低——这种策略被称为任务打包，先前的研究表明，以协调的方式执行是有效的。
androidauthority.com
。 使用 eBPF 是实现这一目标的关键因素。eBPF 允许在运行时注入自定义内核级代码，并由内核的 BPF 验证器进行安全性验证。这意味着电源策略可以即时更新或完全替换，而无需重启或修补内核。
free5gc.org
系统管理员甚至自动化策略守护进程可以根据不同场景加载不同的 BEAR 策略程序——例如，在低负载情况下加载“省电”策略，在检测到高负载计算时加载“性能”策略，或者针对服务器、移动和嵌入式配置文件分别加载不同的策略。这种动态自适应性已在基于 BPF 的调度环境中得到验证：最近合并的sched_ext框架允许在运行时通过 BPF 加载新的调度策略，从而促进快速实验和针对特定工作负载的调优。
lwn.net
phoronix.com
基于 BPF 的调节器有望为 DVFS/idle 控制提供类似的灵活性。事实上，一个原型BPF cpufreq 调节器（称为cpufreq_ext）已于 2024 年底提出，可在 BPF 程序中实现自定义频率调节逻辑。
lwn.net
lwn.net
这项工作背后的动机与我们的一致：现有的调控器并不能满足所有需求，维护自定义内核模块也很繁重，因此 BPF 提供了一种轻量级、可维护的专业化替代方案
lwn.net
lwn.net
我们的 BEAR 框架扩展了这一理念，将空闲和热管理也整合在一起，而不仅仅是 CPU 频率。值得注意的是，cpufreq_ext RFC 已经预见到了与 BPF 调度器的协调——它明确指出频率调节和任务调度应该协同设计，并且 cpufreq_ext 可以与之协同工作，sched_ext通过协调一致的决策来优化系统响应能力和功耗。
lwn.net
。BEAR 基本上会推广这种协调设计，将空闲状态和热响应纳入循环中。 总而言之，我们的目标是创建一个全面的电源管理调速器 (BEAR)，使用 BPF 驱动的策略统一频率调节、空闲状态控制和热管理。我们相信，通过消除不同控制器之间的盲点和拉锯战，这可以显著提升各种场景下的能效和性能稳定性。这项工作的贡献如下：
统一电源管理策略：我们设计了一个统一的调速器，它可以对 CPU 的 DVFS（P 状态）和空闲（C 状态）进行联合决策，并主动管理热余量，而不是依赖于各个子系统按顺序或被动地运行。这将使交互更加合理——例如，在不考虑热余量的情况下绝不提升频率，并智能地选择通过降频节流还是强制空闲注入来控制温度。
androidauthority.com
arxiv.org
通过将性能状态和空闲状态视为同一功率性能连续体的两个方面，我们的方法可以利用基于实时条件的“竞速至空闲与减速”决策等机会。
基于 BPF 的可扩展性：我们在内核中引入了一个 BPF 挂钩的调控器框架（基于 cpufreq_ext 概念
lwn.net
对于 CPU 管理器也有类似的想法
jauu.net
这使得策略逻辑可以在一个高级的、内存安全的 BPF 程序中实现，并且可以动态更新。它大大降低了实现专门电源策略的门槛——无需重新编译内核或编写树外模块。
lwn.net
lwn.net
研究人员和开发人员可以快速制作新策略（甚至是基于机器学习的策略）的原型，并将其动态部署到生产系统中
研究.google
free5gc.org
。BEAR 本质上将“可编程调度程序”的理念引入了电源管理，促进了类似于 eBPF 彻底改变网络和跟踪的创新。
CPU-GPU 协调管理：虽然我们主要关注 CPU 核心，但我们将扩展框架，使其同时考虑GPU 的频率和功耗，尤其适用于集成 SoC（移动和嵌入式平台）。BEAR 的策略可以考虑 GPU 的利用率和温度，从而在 CPU 和 GPU 之间分配功耗预算，以实现最佳整体性能。先前的研究表明，协调管理 CPU 和 GPU 可获得更好的结果——例如，当 GPU 成为瓶颈时，降低 CPU 速度以允许 GPU 不受阻碍地运行，反之亦然。
androidauthority.com
我们将以 Park等人的Co-Cap（CPU-GPU 合作上限）和ML-Gov调控器等研究为基础，这些调控器通过协调 CPU big/LITTLE 核心和 GPU 的 DVFS 实现了节能游戏
arxiv.org
arxiv.org
。BEAR 将推广这些想法，并可能提供一个通过 BPF 实现此类策略的平台。
自适应且负载感知的策略：利用实时遥测技术（例如性能计数器、任务利用率信号、热传感器），BEAR 可以实施适应负载阶段和需求的策略。例如，它可以检测内存受限阶段和计算受限阶段，并相应地调整频率（正如 Hsieh等人针对内存感知 DVFS提出的建议）。
arxiv.org
它还可以通过提升或限制某些工作负载的功率来强制执行QoS约束，类似于涡轮模式使用的“task_boost”概念
lwn.net
或热感知QoS框架中的热QoS管理
arxiv.org
。这种动态适应性将被构建到我们的 BPF 策略模板中，可能利用可随时间更新的机器学习模型或启发式方法。
开源原型与评估：我们将以 Linux 内核补丁的形式实现 BEAR（旨在最终提交上游讨论），并提供一套参考 BPF 调控器程序（例如，性能导向策略、均衡策略等）。我们还将提供一个评估套件，用于分析各种场景下的功耗、性能和热行为。通过开源该框架，我们希望能够吸引学术界和业界（操作系统开发者、芯片供应商）共同探索统一电源管理。
最终，BEAR 的影响将在于提升各种设备的系统效率（延长电池寿命，降低散热成本）和性能一致性（避免突然的热节流或延迟峰值）。它顺应了节能系统软件的发展趋势，并有望成为电源管理学术进步与 Linux 内核实际部署之间的桥梁。
相关工作和背景
Linux 电源管理子系统（DVFS、空闲、热）
Linux 历来将 CPU 电源管理模块化为独立组件。CPUfreq提供了诸如“ondemand”、“performance”之类的调速器，以及较新的调度程序驱动的schedutil，它可以根据利用率或其他启发式方法调整 CPU 电压/频率。
lwn.net
另外，CPUidle控制 CPU 在无工作时进入硬件睡眠状态（C 状态）；它具有“阶梯”（简单的线性空闲状态进程）和“菜单”（根据预期空闲持续时间选择空闲状态的预测性调节器）等调节器
lwn.net
lwn.net
。Linux 中的热管理由热子系统处理，该子系统监控温度传感器（热区）并使用称为热调节器的策略（例如“step_wise”或高级“power_allocator”，本质上是 ARM 的 IPA）来启动冷却设备（如风扇或 DVFS 限制器）。
androidauthority.com
androidauthority.com
这些系统以最低限度的协调运行。如前所述，调度程序基本上不知道 cpufreq 和 cpuidle 的决策。
lwn.net
直到最近几年才有所改变。例如，cpufreq 调速器传统上测量 CPU 的空闲时间和负载来确定频率，但并不知道CPU 空闲的原因（可能是没有任务，或者任务被故意移动了）。
lwn.net
androidauthority.com
同样，cpuidle 调控器会根据计时器和中断历史记录预测空闲时长，但它不会与 cpufreq 通信——即使 cpufreq 只是降低了频率以节省电量，它也可能决定深度休眠，从而不必要地加倍延迟损失。热控制仅在温度超过阈值时启动，通常是作为最后的手段，它们通常会向 cpufreq 发送信号（限制最大频率），甚至强制空闲注入以冷却 CPU。
docs.kernel.org
people.cs.pitt.edu
这种被动的热节流可能会导致性能突然下降。在当前的主流 Linux 中，存在一些间接的相互作用——例如，schedutil调控器使用调度程序提示（任务利用率和空闲时间） 。
jauu.net
并且 EAS 修改（在具有能量模型的 ARM 平台上启用时）会影响任务放置，从而有利于提高能源效率
androidauthority.com
。此外，IPA 等热框架可以有效地与调度程序的负载跟踪相结合，以在 CPU 和 GPU 之间重新分配热预算
androidauthority.com
androidauthority.com
尽管如此，策略逻辑仍然分散在各个子系统之间。2015 年内核文档草案中有一段引人深思的文字：“调度程序几乎完全不知道 cpuidle 和 cpufreq 控制器在做什么”，强调了没有集成。
lwn.net
。 Linux 社区的研究和讨论长期以来一直倡导更深层次的集成。功耗感知调度(Power-Aware Scheduling)工作（大约在 2012-2014 年）演变成了统一功耗驱动程序的概念，正如 Morten Rasmussen 等人所描述的那样。
lwn.net
lwn.net
。该电源驱动程序 API 将允许调度程序以统一的方式查询和控制电源状态 - 例如，best_wake_cpu()选择最节能的 CPU 来唤醒（优先选择集群中已经唤醒的 CPU），go_faster()/go_slower()请求性能更改而不指定确切频率，并enter_idle()指示 CPU 进入睡眠状态
lwn.net
lwn.net
。在 Rasmussen 的原型中，这些调用被连接到底层 cpufreq/cpuidle 实现（例如，go_faster()只需调用 cpufreq 驱动程序即可增加频率）
lwn.net
长期愿景是消除单独的 cpufreq 和 cpuidle 子系统，并将其功能纳入此电源驱动程序，而该驱动程序又将由调度程序策略驱动
lwn.net
还设想了一些附加功能，例如get_best_sleep_cpu()选择接下来让哪个 CPU 进入睡眠状态，以及task_boost()在可能的情况下优先对特定 CPU 进行加速（适用于允许一个核心在其他核心空闲时加速的非对称系统）
lwn.net
尽管这种设计很有前景（而且它的一些元素也为 EAS 提供了参考），但它很复杂，也面临质疑——例如，调度程序维护者 Peter Zijlstra 担心它是“一堆随机的钩子，没有整体框架”。
lwn.net
最终，该概念的部分内容以更渐进的方式进入主线：EAS（围绕 ARM64 的 Linux 4.14 合并）引入了能量模型，并向调度程序传授了 CPU 容量和能量成本
开发者.arm.com
androidauthority.com
; schedutil（在 Linux 4.7 中）提供了一个调度程序驱动的 cpufreq 调控器，其中调度程序的负载跟踪直接影响频率决策
jauu.net
；并且TEO空闲调控器（Linux 5.3）用一个更简单但更准确的模型取代了菜单，该模型专注于下一个计时器事件
lwn.net
尽管如此，这些都是单独的集成，而不是完全的统一——cpufreq 和 cpuidle 仍然是独立的子系统，具有自己的调节器和可调参数。 关于动态电压频率响应 (DVFS) 和电源管理的学术研究涵盖算法优化和协调控制。许多研究探讨了动态电源管理 (DPM)（即策略性地使用空闲/睡眠状态）与 DVFS 的结合。例如，最早的理论研究提出了 DVFS+DPM 的组合调度问题，以在性能受限的情况下最大限度地降低功耗。
es.cs.rptu.de
研究人员提出了调度算法，该算法可以决定核心频率以及何时关闭核心，通常用于实时系统以最小的能量满足最后期限
es.cs.rptu.de
mdpi.com
这些方法包括最优控制理论方法以及实用启发法。一项研究比较了三种策略——单独使用 DVFS、单独使用 DPM（空闲）以及集成方法——结果发现，在许多工作负载下，组合策略的表现优于单独使用任何一种策略。
es.cs.rptu.de
这反映了我们的前提，即同时控制频率和空闲状态比单独控制其中一种机制能产生更好的效果。竞速至空闲模式与 DVFS 的争论很经典：是以最大速度运行并快速完成（然后进入空闲状态）更好，还是以较慢的速度运行以节省能源但运行更长时间更好。答案通常取决于功耗如何随频率变化以及睡眠状态的盈亏平衡成本。统一的调速器基本上可以即时决定这一点。Kim等人最近为移动设备引入了“零热节流”DVFS (zTT) ——一种基于学习的 DVFS 控制器，它可以调节频率以使设备保持在热限值以下，从而避免任何强制热节流事件
arxiv.org
这正是热感知统一调速器可以实现的那种细致入微的策略（牺牲一些性能以防止未来出现更严重的性能下降）。类似地，Sahin等人的Maestro框架 (2019) 通过动态限制功耗来管理热约束下的应用程序 QoS，以满足热预算，同时又不违反性能目标。
arxiv.org
这些工作强调了热感知在 CPU 频率控制中的重要性——这通常由 Linux 中单独的热调节器处理，但 BEAR 将其直接纳入其决策中。 在CPU-GPU 协调方面，移动 SoC 上已有大量的研究成果。移动游戏和 3D 应用会同时使用 CPU 和 GPU，因此提高能效需要平衡两者。Hsieh等人(ESTIMedia 2015) 提出了一种适用于移动游戏的内存感知型 CPU-GPU 协同 DVFS 调速器，该调速器能够根据 GPU 负载和内存带宽利用率同时调整 CPU/GPU 频率。
arxiv.org
例如，他们的调速器可以检测 CPU 何时等待 GPU 或 GPU 何时等待 CPU，并降低等待组件的频率以节省能源，同时不影响性能。Park等人（2015 年）提出了Co-Cap技术，这是一种以协调的方式限制 CPU 和 GPU 频率以满足游戏工作负载的能源预算的技术。
arxiv.org
。Park等人(2017) 在ML-Gov中对此进行了扩展，他们使用机器学习来预测给定工作负载阶段（游戏场景）的最佳 CPU/GPU 频率对
arxiv.org
ML-Gov 将 CPU–GPU DVFS 设置视为联合优化问题，据报道，与默认调控器相比，它显著提高了 big.LITTLE 智能手机游戏的能效
arxiv.org
最近，Park 及其同事（Karzhaubayeva等人，2023 年）研究了嵌入式 CPU-GPU 平台上的CNN 工作负载以及集成 DVFS 策略的优势
arxiv.org
。他们表明，不同的神经网络层（计算密集型与内存密集型）受益于 CPU/GPU 上不同的 DVFS 设置，并且集成方法可以比静态或仅限 CPU 的调速器产生更好的能量/性能平衡。共同的主题是，由于CPU 和 GPU 之间共享功率/热预算和工作负载依赖性，协调 CPU 和 GPU 频率调速器产生比独立控制更好的结果。Linux Android 系统通常在用户空间实现这种协调（Android Power HAL或供应商热管理器将根据启发式方法同时提升或限制 CPU/GPU）。我们的 BEAR 框架将能够以更有原则的方式在内核级别实现这些策略——可能使用 ML 模型作为 BPF 程序的一部分来预测最佳频率组合，类似于 ML-Gov 的方法
arxiv.org
值得注意的是，最近一项关于移动 AI 推理的研究指出，默认的移动 DVFS 调节器效率低下，通过联合调整 CPU、GPU 和内存频率，可以在相同的能源成本下大幅减少推理延迟（最多可减少 60%）。
arxiv.org
arxiv.org
这表明我们的工作还有很大的改进空间。 可扩展内核调度和 BPF：还有一些相关工作致力于提高内核策略的适应性。Google 的ghOSt项目（发布于 SOSP 2021）通过内核框架将调度决策委托给用户空间代理。
研究.google
研究.google
ghOSt 允许开发者在用户空间（使用任何语言）编写自己的调度算法，而内核则提供通信和执行机制。其动机与我们类似：内核调度/功耗策略是硬编码的，难以更改，但定制策略可以显著提升特定工作负载的性能或效率。
研究.google
通过开放自定义调度程序接口，ghOSt 实现了低开销，并实现了对上游通用代码不切实际的优化
研究.google
研究.google
我们的方法不同之处在于，我们在内核中使用 eBPF，而不是在用户空间进程中使用，但其核心思想——使操作系统策略可编程——是相同的。eBPF 已成功应用于网络（XDP、tc BPF）、安全（Linux 安全模块 BPF hooks）、跟踪/监控sched_ext（BCC、bpftrace），以及最近在 Linux 6.7 中引入的调度。
phoronix.com
sched_ext本质上是 ghOSt 的 BPF 对应物：它允许 BPF 程序定义新的调度类（包含辅助函数以及用于 enqueue、pick_next_task 等的钩子）。Hao Luo等人在 2020 年 LPC（Linux Plumbers 大会）上演示了 BPF 调度器原型，表明即使是复杂的调度策略也可以在运行时实现和切换，并且开销极小，无需重建内核。
free5gc.org
研究.google
随后，Yipeng Zou 在 2024 年的 RFC 补丁中引入了cpufreq_ext一个基于 BPF 的 CPU 频率调节器
lwn.net
补丁定义了一组 BPF 钩子函数（get_next_freq等等），可以在频率改变事件发生时决定下一个频率
lwn.net
。Zou 的设计与集成，sched_ext以便 BPF 调度器和 BPF 频率调节器可以紧密协调
lwn.net
这项工作为 BEAR 开创了一个强有力的先例，证明了 (a) BPF 用于电源管理的可行性，以及 (b) 内核社区对此类解决方案的兴趣（该研究已发布在 linux-pm 和 bpf 邮件列表中征求反馈）。此外，研究人员还在探索 eBPF 用于空闲状态控制——例如，作为“生态计算”计划的一部分，研究人员开发了一个基于 eBPF 的 CPUIdle 调控器原型，允许在内核空间中设置动态、有针对性的空闲策略。
jauu.net
。虽然这只是一项实验性的努力，但它与我们的目标非常一致（我们打算将这种 BPF 空闲控制与 BPF DVFS 控制统一起来）。 总而言之，BEAR 正处于多项发展的交汇点：Linux 对节能调度的推动、协调 DVFS/DPM 和 CPU-GPU 管理的学术研究，以及 eBPF 作为内核策略定制工具的兴起。据我们所知，目前尚无任何研究能够提供一个统一的、BPF 可扩展的调控器，能够同时涵盖频率、空闲和热管理。BEAR 将率先实现这一目标。它将以现有技术的理念为基础——例如使用能耗模型（如 EAS 中的方案）和协调 CPU/GPU 控制（如 Park等人的方案）——但以灵活的 BPF 驱动方式实现它们。接下来，我们将详细描述我们提出的设计方案，然后概述评估和强化此方法的研究计划。
建议方法：BEAR 调控器的设计
架构概述： BEAR 的架构由一个内核空间框架以及一个或多个实现该策略的 eBPF 程序组成。该框架将钩住三个主要的内核事件/上下文：(1)任务调度事件– 例如，运行队列状态发生变化（任务添加/移除或平均负载更新）；(2) CPU 空闲进入/退出事件– CPU 即将进入空闲状态或刚刚唤醒；以及 (3)热/功耗事件– 例如，定期温度采样或功耗上限通知。这些事件的钩子将调用 BPF 处理函数，这些函数随后可以做出诸如调整频率或强制空闲之类的决策。具体来说，我们将利用并扩展现有的 BPF 钩子点：cpufreq_ext RFC 已经建立了一个用于频率决策的钩子 ( get_next_freq)。
lwn.net
我们可能会为 cpuidle 引入类似的钩子（例如，get_idle_state(cpu)用于选择空闲状态或在即将唤醒时否决空闲条目）。热事件可以通过将 BPF 附加到热子系统中的跟踪点（例如，当热区跨越跳变点时添加的跟踪点）或通过一个通过 HWMON 接口读取传感器值的计时器来处理。我们还可能会维护一个BPF 映射来在这些钩子之间共享状态——例如，一个数组或哈希映射，用于存储每个 CPU 的最新已知温度或最近的空闲时长历史记录，以供 BPF 策略访问。 BEAR eBPF 程序充当着调控器的大脑。它大致会实现如下逻辑：每当调度事件指示工作负载发生变化时（例如，之前空闲的 CPU 现在有任务了，或者 CPU 总体利用率突然上升），BPF 代码就会预测所需的性能水平，并为受影响的 CPU 设置合适的频率（通过在 cpufreq_ext 钩子函数中返回目标频率）。
lwn.net
同时，它可以决定是否应将更多核心上线或使其处于空闲状态。由于 BPF 程序可以查看整个系统（我们可以允许它读取每个 CPU 的负载、平台的能耗模型等指标），因此它可以实施全局策略——例如，根据所选策略，在效率高的情况下将任务打包到部分核心上（节省未使用核心的功耗），或者在温度问题下分散任务以避免热点。BPF 代码还可以直接操纵空闲行为：一种方法是使用空闲注入（内核已经支持强制空闲周期作为冷却方法）。
docs.kernel.org
）。当我们想要降低功耗或温度时，BEAR 可以计算强制每个 CPU 空闲的时间百分比，有效地像一个“虚拟 DVFS”一样，通过以高频率但低利用率对 CPU 进行占空比循环——在某些情况下，这比降低频率更有效（例如，为了避免限制未充分利用 CPU 的内存密集型任务，最好以全速突发运行该任务，并在其间空闲，在降低平均功耗的同时保持吞吐量）。BPF 策略可以通过在热余量较低时调度空闲注入来实现这种逻辑。
docs.kernel.org
或者，为了选择深度空闲状态，BPF 程序可以覆盖 cpuidle 调控器的选择：例如，如果它知道系统将空闲足够长的时间（基于其能量模型或计时器），它可以选择比默认菜单调控器更深的 C 状态；或者相反，如果预计系统会短暂休眠，则保持较浅的 C 状态。我们可以通过拦截 cpuidle 的决策来实现这一点（或许可以通过对 cpuidle 进行一些小修改来调用 BPF 钩子）。 频率和空闲决策的整合： BEAR 的策略循环将频率和空闲状态同时考虑，这是一个关键的设计方面。实际上，这意味着当 CPU 没有任务时，BEAR 会决定何时对其进行时钟门控/电源门控（空闲），以及唤醒时以何种频率恢复。当 CPU 繁忙时，BEAR 会决定以何种频率运行以及是否使用备用核心。这些相互关联的决策将以能耗模型和成本分析为指导。我们将采用一个类似于 EAS 的简化模型：对于每个 CPU（或集群）状态（频率和空闲状态的组合），我们都有一个估算的功耗和性能容量。利用此模型，我们的 BPF 程序可以估算不同决策对能耗的影响。例如，如果有一个可运行任务和两个核心（一个大核心，一个小核心），BEAR 可以进行比较：以较低的频率在大核心上运行，以最大频率在小核心上运行，还是以更低的频率使用两个核心（如果可并行化）等等，并选择最节能且满足性能需求的选项。
androidauthority.com
这扩展了 EAS 的任务分配理念，明确地将频率调整纳入决策变量（EAS 目前依赖 schedutil 来调整频率）。此外，BEAR 还将考虑热余量：我们的能耗模型可以通过热阻/热容估算进行增强，以预测特定功耗将如何转化为温度升高。如果模型或最近的测量结果表明，所有大核心以峰值运行会在例如 5 秒内触发热节流，BEAR 可以主动限制频率或使用更少的核心来保持温度稳定（类似于 zTT 的先发制人避免节流的方法）。
arxiv.org
）。 GPU 和其他加速器：为了将 BEAR 扩展到 CPU 之外，我们将与devfreq子系统（负责处理 GPU、内存控制器等设备的 DVFS）进行交互。许多移动 GPU 都配备了 devfreq 驱动程序，它会公开可用频率并使用一个简单的调速器（通常是“simple_ondemand”或特定于供应商的）。我们计划也为 devfreq 设备引入 BPF 钩子，或者引入一种通用机制，使 BEAR 的决策可以包含对 devfreq 的命令（例如，将 GPU 设置为某个 OPP（Operating Performance Point，运行性能点））。在我们的原型中，我们可能会对某个 GPU 设备进行特殊处理：BEAR BPF 程序可以读取 GPU 负载（通过现有的内核计数器）和温度，然后根据 CPU 频率决定 GPU 频率。这可能涉及与 GPU 驱动程序的同步。由于实现完整的 devfreq BPF 接口非常复杂，因此一个中间步骤是运行一个用户空间辅助程序，该程序监听 BEAR 的决策（通过 BPF 性能事件或共享映射），并使用现有的 sysfs 旋钮将其应用于 GPU 驱动程序。无论采用哪种实现方式，策略逻辑都旨在在共享功率/热量范围内最大化整体吞吐量。例如，如果 GPU 利用率为 100%，并且接近热量极限，BEAR 可能会降低 CPU 活动（或频率），以便为 GPU 提供更多的热量余量，从而提高总工作量（ARM 的 IPA 描述中提到了这一策略）。
androidauthority.com
相反，如果 CPU 是瓶颈，而 GPU 处于空闲状态，BEAR 可以为 CPU 分配更多算力（甚至更积极地让 GPU 处于空闲状态）。这种协调预算将借鉴 Co-Cap 等先前的研究成果。
arxiv.org
。 BPF 程序结构： BEAR BPF 程序可能包含多个连接到不同钩子的 BPF 函数。对于 cpufreq 决策，我们将get_next_freq(policy)按照 cpufreq_ext 的定义进行实现。
lwn.net
。此函数将根据当前负载和预测负载，计算 CPU 策略（一组核心共享 DVFS）的下一个频率。对于 cpuidle，我们可以实现一个钩子，例如allow_idle(cpu, state)在 CPU 进入空闲 C 状态之前运行，允许 BPF 批准或调整所选状态。对于热力，定时器触发的 BPF（使用 BPF 定时器或内核工作）可以定期调用一个函数来评估温度，并可能发出调整信号（例如降低频率上限）。这些钩子之间的通信可以通过全局 BPF 映射完成（例如，存储 get_next_freq 函数将遵守的全局“max_freq”上限，该上限可由热力监控函数更新）。此外，我们可以使用BPF perf 事件机制将信息提供给用户空间或记录决策以供调试。这些函数内部的逻辑可以根据需要简单或复杂，但需遵守 BPF 验证器的限制。我们将从用 C 语言编写的相对简单的启发式算法开始，并可能集成更高级的逻辑（例如，最佳配置表或类似于机器学习方法的 DVFS 小型神经网络），作为后续的增强功能。值得注意的是，eBPF 存在一些局限性（例如，循环必须有界，并且存在指令数限制），但这些局限性一直在不断扩展，并且一些技术，例如在 BPF 和用户空间之间拆分逻辑（就像一些 eBPF 调度器原型中所做的那样），也正在不断改进。
free5gc.org
free5gc.org
) 可以帮助减轻复杂性。一种有前景的方法是混合模型：让 BPF 处理内核中的快速决策（例如，快速响应事件），但将更繁重的计算或长期策略调整委托给用户空间守护进程。例如，用户空间进程可以监视系统行为并更新 BPF 策略的某些参数（通过 BPF 映射），甚至可以为新阶段加载新的 BPF 程序。这类似于 Facebook 的 BPF 调度器如何与用户空间配对（例如，scx_rustland在用户空间执行复杂的调度，并仅将 BPF 用于快速入队/出队钩子）。
free5gc.org
free5gc.org
)。在 BEAR 的案例中，用户空间助手可能会运行机器学习模型来预测即将到来的工作负载变化（例如，根据最近的趋势接近热阈值）并向 BPF 代码发出信号以调整积极性。 安全性和回退：由于 BEAR 会控制关键硬件参数，我们必须构建安全措施。一项安全措施是始终遵守硬件强制的限制：例如，不超过 CPU 的最大频率（BPF 程序将被限制输出硬件/固件允许范围内的频率）。同样，如果我们的策略出现问题或崩溃，系统应该恢复到正常的默认值。我们将实现一种机制，如果 BPF 程序被移除或遇到错误，内核会回退到标准调控器（可能是性能调控器或 schedutil）。cpufreq_ext 补丁已经指定，如果未加载 BPF 调控器，它将作为性能调控器运行。
lwn.net
我们将把这个想法扩展到空闲和热负载——例如，我们可能会将常规的热负载触发点操作作为最后的手段（这样，如果我们的 BPF 无法降低温度，现有的关键触发点仍将关闭或节流以保护硬件）。我们还将测试 BEAR 的开销是否最小：eBPF 的设计注重效率，但我们会测量钩子的执行时间。先前的研究表明，BPF 调度钩子可以非常快（以微秒为单位），并且对整体吞吐量的影响可以忽略不计。
研究.google
我们期望相同，但如果需要，我们可以降低钩子频率（例如，不要在每个周期都调用 get_next_freq，最多每隔一段时间调用一次）。cpufreq 采样可以采用事件驱动（在任务唤醒/睡眠时），而不是轮询，这样效率更高，响应速度更快。
lwn.net
。 总体而言，BEAR 的设计融合了内核电源管理和基于 BPF 的定制化理念。图 1（概念图）展示了 BEAR 的运行方式：BPF 程序位于核心，接收输入（调度器信号、空闲事件、温度）并发出决策（设置频率、空闲提示、热容上限），这些决策随后会影响 CPU 驱动程序和调度器。这个闭环反馈机制会根据当前情况持续调整系统的电源状态。 （我们提供了一个描述性示例，而不是图片）：假设我们使用 BEAR 在 big.LITTLE 移动 SoC 上运行一款游戏。最初，两个 CPU 集群都处于低功耗空闲状态。游戏启动后，BEAR 的调度程序钩子检测到大量任务被唤醒。BPF 策略知道这是一个对延迟敏感的应用（可能通过进程名称或 cgroup 标识），因此它会立即调用go_faster：它将大核设置为高频率，并使一个大核退出空闲状态（如果它处于深度空闲状态，我们会选择浅度空闲状态或保持其运行），以处理主线程。
lwn.net
它还会将一些后台任务优先分配给小核心。随着游戏帧数的渲染，BEAR 会通过 devfreq 统计数据监控 GPU 使用情况；当注意到 GPU 负载过高且温度升高时，BEAR 会略微降低大核心频率（牺牲一些不必要的 CPU 性能余量，以减少发热量），并且如果某个大核心未得到充分利用，可能会使其处于空闲状态。
androidauthority.com
热 BPF 函数会计算出这使得设备保持在热跳变点以下 - 避免在没有 BEAR 的情况下设备可能达到热跳变点并严重限制 GPU，从而导致帧率下降的情况。在加载屏幕（CPU 密集阶段，很少使用 GPU）期间，BEAR 会做相反的事情：它允许 CPU 短暂地加速（因为现在温度可以接受）但会完全停止 GPU 和一些 LITTLE 核心（深度空闲）以节省电量。在这样做的过程中，BEAR 基本上会协调所有组件，以实现每个时刻的最佳每瓦性能。所有这些都不需要为特定游戏编写内核代码 - 它由我们编写的通用 BPF 策略处理，该策略可以随着时间的推移而改进，甚至可以通过更新部署给用户（无需升级内核）。
研究计划和方法
我们将分几个阶段实施该项目，与上述设计保持一致： 第一阶段：BPF 调控器基础设施。我们将开发 BEAR 的核心内核支持。这涉及扩展 cpufreq_ext RFC。
lwn.net
以满足我们的需求，并为空闲和热负载编写新的钩子。我们将以最新的 Linux 内核（>= 6.5）作为基准。此阶段的关键里程碑：(a) 使一个简单的 BPF 频率调节器能够正常工作（例如，在 BPF 中复制保守或按需调节器的功能）。我们可以基于 Y. Zou 的 cpufreq_ext 补丁进行构建
lwn.net
，并根据需要根据我们的内核版本进行调整。（b）创建一种通过 BPF 覆盖空闲决策的机制。一种方法是在 cpuidle 框架中进行微小的更改，使其调用 BPF 程序来选择空闲状态，而不是使用菜单/TEO。我们可以将其实现为一个新的 cpuidle 调控器“bpf_idle”，它只是将决策推迟到 BPF 钩子。（c）设置热监控：可能是/sys/class/thermal定期读取并调用 BPF 的内核线程，或者使用powercap接口（用于服务器上的英特尔 RAPL）来获取功耗。如果直接挂钩很复杂，我们可以首先依靠用户空间通过映射将热信息传递给 BPF（例如，守护进程读取温度并更新调控器参考的 BPF 映射）。第 1 阶段的成果将是一个补丁，允许 BPF 程序控制 CPU 频率和空闲状态（并接收热数据）。我们将在一个平台（例如 x86 台式机或 ARM 开发板）上测试基本功能——验证我们是否可以加载 BPF 调控器，以及它是否可以成功更改频率并影响空闲进入。我们将通过检测钩子（使用 ftrace 或 BPF 本身来计时 BPF 的运行时间和频率）来测量开销。预期即使使用简单的策略，开销也很低；早期的研究表明，BPF 直接函数调用（通过bpf_struct_ops）可以非常高效。
phoronix.com
lwn.net
。如果出现任何性能问题，我们将进行调整（例如，降低挂钩频率或卸载计算）。 第二阶段：策略开发（以 CPU 为重点）。基础设施到位后，我们将在 BPF 中开发实际的电源管理策略。我们计划创建至少两个版本的策略：(i)基于已知良好实践的启发式策略（例如，类似于 EAS + PID 热控制）；以及 (ii) 作为延伸目标的更高级的机器学习策略。对于启发式策略，我们将编码如下逻辑：如果 CPU 利用率 > X 且温度 < Y，则逐渐提高频率；如果利用率下降或温度上升，则降低频率；如果一个 CPU 处于空闲状态，而另一个 CPU 过载，则考虑唤醒空闲的 CPU（除非能耗损失过高）；使用能耗模型数据来决定何时整合负载还是分散负载，等等。我们可能会利用现有的 schedutil 调控器公式，但通过热感知对其进行增强——本质上是集成一个热阻尼项，以避免超过热限值。
arxiv.org
空闲逻辑将包含最近的空闲时长跟踪：例如，在 BPF 中实现一个简化版的 TEO（它查看下一个计时器事件和最近的空闲时长
lwn.net
)。BPF 代码可以维护每个 CPU 的最近空闲时间记录，并以此预测即将发生短空闲还是长空闲，然后据此确定空闲状态。对于热管理，一个简单的策略是：当温度超过一个高阈值时，设置最大频率上限（并可能强制大核心进入空闲状态），直到温度冷却到一个低阈值（滞后）。这模仿了 Linux 的 step_wise 调控器，但我们可以根据与 Tjmax（结点最高温度）的接近程度，按比例进行限制，使其更加平滑。我们将在实际工作负载和调优参数上评估这些启发式规则。 对于基于机器学习的策略，如果时间允许，我们可以训练一个模型（例如轻量级决策树或神经网络），该模型以当前状态（利用率、温度等）作为输入，并输出最佳频率设置。Kim 等人的深度强化学习在多核热管理中的应用（CARTAD，2022）是一个启发，该模型使用神经网络根据利用率和温度读数选择 DVFS 操作。
arxiv.org
我们可能无法实现完整的强化学习代理，但可以使用离线训练：运行各种工作负载，收集良好的功耗-性能行为的痕迹，并训练一个模型来模拟这些行为。该模型（如果足够小）可以作为一系列条件语句或查找表嵌入到 BPF 逻辑中（因为 BPF 不易支持浮点或大型矩阵）。或者，一个更简单的预测模型可能就足够了——例如，使用 BPF 可以计算的功耗和性能的线性回归公式。 第三阶段：扩展至 GPU 和多组件协调。我们将扩展该策略以处理 GPU DVFS（在可用平台上），并更深入地探索多核协调。我们将在配备 GPU 和片上热传感器的 ARM big.LITTLE SoC（可能是开发板或智能手机）上进行测试。我们计划集成 GPU 的 devfreq。我们可能会为 devfreq 实现一个类似于 cpufreq_ext 的虚拟 BPF 调控器（即使不完整，至少也要实现一个定期调用以设置 GPU 频率的钩子）。如果编写 devfreq BPF 钩子过于繁琐，我们可以使用用户空间代理方法：BPF 程序可以发送所需的 GPU 性能级别信号（例如，通过 BPF 映射值 0-100），然后一个用户空间守护进程（具有 root 权限）监视该映射，并通过现有的 sysfs 接口（例如写入/sys/class/devfreq/../min_freq或使用供应商 IOCTL）调整 GPU 频率。这不如纯内核控制那么优雅，但它使我们能够原型化统一 CPU-GPU 策略的效果。我们将实施以下策略：将 CPU+GPU 功耗保持在总预算以下；如果 GPU 已饱和且温度升高，则进一步降低 CPU 功耗，反之亦然。我们将使用特定工作负载（例如，3D 游戏或 GPU 计算任务以及 CPU 后台任务）来验证这些策略。此外，在此阶段，我们将研究big.LITTLE 上的每个集群策略：决定如何在 big 核心和 LITTLE 核心之间分配负载。我们的统一策略可以直接做出该决策（不同于原生 Linux，它依赖于任务调度程序来执行，然后由 cpufreq 做出反应）。我们可能会实施这样的逻辑：“如果 LITTLE 核心接近最大利用率且性能不足，则将任务迁移到更高频率的大核心”或相反的逻辑：“如果大核心在高频率下大部分时间处于空闲状态，则将任务迁移到 LITTLE 核心”——这实际上是一种简化的动态任务迁移，通常由 EAS 处理。
androidauthority.com
。如果需要，我们将与调度程序协作来实现迁移（也许通过在用户空间中使用 Linux 的亲和性或 cgroup 接口来固定实验任务，因为除非使用，否则在 BPF 中执行此操作很复杂sched_ext）。 阶段 4：实验评估。我们计划在至少三个测试平台上进行广泛的实验，以涵盖以下范围：(a)移动级平台（例如 Raspberry Pi 4 或搭载 big.LITTLE 的 ODROID 开发板，或基于骁龙的开发设备），运行交互式和多媒体工作负载；(b)服务器级系统（x86 服务器或高核数台式机），运行面向吞吐量和突发性工作负载；以及 (c)嵌入式系统（如果我们使用 Pi，则可能与 (a) 重叠，或者像 NVIDIA Jetson TX2 这样拥有 CPU+GPU 并常用于边缘计算的设备）。我们将比较以下设置：BEAR 调速​​器、默认 Linux以及（如果可能）现有的已调优解决方案。对于默认 Linux，我们将使用 schedutil 调速器作为 cpufreq 调速器，使用 TEO 作为 cpuidle 调速器，并使用标准热调速器（根据平台使用 IPA 或 step_wise）。如果使用 ARM，我们将启用 EAS（这将使 Linux 发挥最佳效率）。对于调整基线，在移动设备上我们可能包括 Android 的默认设置（可能是交互式调节器 + 供应商热限制），在服务器上可能包括“省电”调节器（以查看能量与性能极限）或任何可用的功率上限机制（如功率限制模式下的英特尔 RAPL）。 工作负载：对于移动/嵌入式系统，候选工作负载包括：3D 游戏或基准测试（强调 CPU-GPU 协调）、Web 浏览模拟（测试交互式响应能力和短时间突发）、视频播放或 AR 应用程序（中等、稳定负载，热稳定状态），以及可能的 AI 推理任务（例如在 CPU/GPU 上运行小型神经网络，以了解我们如何管理持续负载）。对于服务器，我们将使用以下工作负载：SPEC CPU 基准测试（测量功率限制下的吞吐量）、OLTP 数据库模拟（具有突发和空闲周期，测试我们可以多快地调高/调低速度），以及可能处理不同请求负载的 Web 服务器（评估延迟与功率之间的权衡）。我们还将测试专门针对每个子系统的微基准测试：例如，CPU 绑定的微基准测试，以查看 BEAR 与 schedutil 如何提高频率；空闲循环测试，以测量空闲功耗和唤醒延迟；热步进测试（以最大速度运行 CPU+GPU 直到热节流，比较节流的优雅程度）。 指标：我们将收集一系列指标：(1)性能：例如，批处理作业的执行时间、游戏的帧率、交互式任务的延迟、服务器工作负载的吞吐量（每秒请求数）。(2)能耗：使用硬件功率计（如有）。在移动设备上，我们可能会使用 Monsoon 功率监视器或内部电量计来记录电池消耗。
arxiv.org
在 x86 上，我们可以使用 RAPL 计数器来测量 CPU 封装能量
lwn.net
我们将计算每次测试的总能耗和平均功率。(3)热行为：我们将记录 CPU（如果适用，也记录 GPU）随时间的变化，并统计热节流的发生次数。理想的结果是，BEAR 将温度保持在临界阈值以下，并将波动降至最低，而默认设置可能会反复达到节流点（导致性能突然下降）。(4)策略开销和稳定性：我们将测量 BPF 处理程序所花费的 CPU 时间（这应该只是单个核心的一小部分），并检查是否存在任何不必要的开销。我们还将通过运行长时间的工作负载来测试稳定性，以确保我们的 BPF 或内核修改中没有内存泄漏或问题。 我们将分析这些结果，量化 BEAR 的优势。例如，我们期望证明，在给定的性能水平下，BEAR 相比现状能够降低能耗。在 CPU 密集型基准测试中，BEAR 可能会在不需要充分发挥性能时，通过智能地闲置核心或限制频率来节省能耗，而 schedutil 可能会以最大速度运行所有核心，因为它没有考虑全局能耗最优。我们预计能量延迟积 (EDP)或每瓦性能等指标将有所提升。在热性能受限的场景下（例如移动设备持续的高负载），BEAR 应该能够避免严重的热节流，从而通过主动控制温度有效地平衡性能，从而实现更高的持续性能。
arxiv.org
我们将使用 BEAR 与默认设置的频率/温度随时间变化的图表来说明这些场景，结果显示默认调节器会超调，然后急剧降频，而 BEAR 则会在热极限附近保持稳定状态（从而获得更高的平均性能）。我们还将检查任务延迟：对于交互式工作负载，BEAR 的统一策略可以通过在需要快速响应时防止深度空闲，或通过比默认设置更快地提升频率来降低​​延迟。例如，我们可以测量从空闲状态唤醒并完成计算的时间（这涉及 C 状态退出延迟和 DVFS 延迟）。如果 BEAR 成功协调这些因素（例如，如果预测到短暂睡眠，则将核心保持在较轻的 C 状态，或者使用“唤醒提升”在唤醒时提高频率），它的性能可能会优于默认设置，因为 cpuidle 可能会无意中造成额外的延迟。
lwn.net
lwn.net
。 我们将使用统计方法来确保显著性，多次运行每个测试并计算均值和方差。BPF 本身的任何开销都将通过测量 BEAR 激活时与正常调速器激活时的“空闲”功耗来捕获，预期差异可以忽略不计。 平台特定考虑：我们注意到不同平台具有不同的特性。在移动 SoC中，DVFS 转换相对较快（在配备硬件支持 DVFS 的现代 ARM 上，转换速度以微秒为单位），并且热容量较低（无主动冷却），因此策略必须非常灵敏，甚至可能采取保守措施以避免过热。此外，移动 SoC 通常具有固件管理的升压机制（例如，ARM CPU 中的快速响应功能会在中断时暂时升压）——我们将考虑这些机制如何相互作用或是否应该绕过它们。在数据中心服务器中，通常有主动冷却（风扇）和更高的功率预算，但也有更多内核。内核之间的协调对于避免功率争用至关重要（如果许多内核处于活动状态，英特尔的 Turbo Boost 会自动降低频率以保持在 TDP 范围内）。 BEAR 可以通过理解工作负载的重要性来改进硬件的默认设置：例如，在运行延迟关键型服务和批处理作业的服务器中，BEAR 可以向延迟关键型核心分配更多处理能力，并对批处理作业核心进行比硬件默认设置更大的节流。我们将模拟此类场景。另一个区别是：英特尔服务器通常使用intel_pstate驱动程序而不是通用的 cpufreq。intel_pstate 拥有自己的内部算法，并且可以在准硬件控制模式（Intel Speed Shift）下运行。对于我们的项目，我们可能需要禁用 intel_pstate（或在被动模式下将其与 cpufreq 一起使用）才能通过我们的调速器获得完全控制权。我们将记录这些调整。在嵌入式系统中（例如运行 Linux 的物联网设备或汽车系统），实时行为可能是一个值得关注的问题。我们将讨论如何调整 BEAR 以实现确定性，例如，通过设置 BPF 的实时优先级或限制其状态更改方式以避免抖动。我们还可以包含一个实时基准（如用于延迟的循环测试或简单的控制循环截止测试），以确保 BEAR 不会违反 Linux 默认的时间限制（事实上，通过减少频率振荡，它甚至可能有助于不总是最大化 CPU 的 RT 任务的稳定性）。 通过这些实验，我们希望找出 BEAR 的优势和可能的弱点。例如，在某些工作负载下，默认的 Linux 调控器可能已经接近最优，而 BEAR 仅带来些许提升——我们也会报告这些情况，因为它们表明统一方法在哪些方面最有效。相反，我们将重点介绍 BEAR 表现优异的场景：可能的混合工作负载、热受限情况以及使静态调控器难以理解的工作负载变化。如果出现任何问题（例如，在极高频事件情况下的 BPF 开销），我们将通过改进方法（例如使用采样）来解决。到目前为止，我们的预期是，通过精心设计，BEAR 能够在不牺牲性能的情况下，在能效方面始终匹敌甚至超越默认策略。
潜在挑战和风险缓解
开发 BEAR 是一项雄心勃勃的工程，涵盖内核修改、BPF 编程和系统评估。我们概述了主要挑战以及应对这些挑战的计划：
内核复杂性和稳定性：修改低级电源管理代码 (cpufreq/cpuidle) 可能会危及系统稳定性。我们必须确保修改不会引入竞争条件或死锁（例如，在 CPU 转为空闲状态时操纵频率）。我们将尽可能简化设计，例如，依赖 cpufreq 中现有的同步机制（该机制已经能够处理并发频率更改请求）。BPF hooksbpf_struct_ops将在定义的点进行集成；如果 BPF 程序出现异常（例如，返回超出范围的频率或无效的空闲状态），我们将对其进行钳制或忽略，以保持稳定性。我们将进行广泛的压力测试（以最大负载运行数小时，快速切换工作负载）以清除错误。作为故障保护措施，我们可能会实现一个看门狗，用于监控系统是否保持在安全的热性能界限内——如果 BEAR 性能不佳（例如导致持续 100°C 或严重的性能损失），系统可能会自动回退到已知安全的调速器。这与 Android 的热守护进程在出现问题时覆盖的原理类似。
BPF 程序限制： eBPF 程序必须经过验证，并在严格的限制（最大指令数、无无界循环、无浮点）内运行。我们的策略可能涉及复杂的计算（例如，求解优化问题或运行机器学习模型）。BPF 代码可能过大或达到验证器限制。为了缓解这个问题，我们将逻辑分解为多个辅助函数，并在需要时使用尾调用（BPF 尾调用允许链接多个 BPF 程序，每个程序都在大小限制内）。我们还将简化公式（预先计算常量、使用非线性函数查找表等）。如果绝对必要，部分逻辑可以驻留在用户空间（BPF 仅执行快速执行）。基于现有技术，我们相信一个相当复杂的调控器可以适用于 BPF——例如，cpufreq_ext 示例在 BPF 中轻松实现了“VIP 任务提升”策略。
lwn.net
和 BPF 调度器都实现了多运行队列平衡逻辑，这可以说比我们的 DVFS 算法更复杂。我们将仔细检查验证器的输出，并相应地调整我们的实现。
性能开销：虽然 eBPF 速度很快，但理论上，频繁运行 BPF 程序（例如，在每个调度程序 tick 或每个空闲条目上）可能会增加开销，尤其是在多核系统上。我们将量化开销，并在必要时限制调用率。例如，我们可能不需要在每次任务切换时重新计算频率——也许每 5 毫秒检查一次就足够了。cpufreq 子系统在 ondemand 调控器中的采样间隔约为 10-20 毫秒。
lwn.net
，并且 schedutil 会在调度事件上触发，这些事件大致发生在任务迁移或唤醒粒度（每秒可能数千次）。我们将找到一个最佳点来调用 BEAR 的逻辑，而不会过高的频率。此外，eBPF 在某些情况下会在原子上下文中运行；我们的设计必须避免在这些上下文中执行昂贵的操作。cpufreq_ext 钩子在 cpufreq 驱动程序调用中运行——只要我们不休眠，这应该没问题。我们的 BPF 将避免任何阻塞调用（BPF 无论如何都禁止阻塞调用）。对于任何可能需要休眠的工作（例如与用户空间交互或等待某些操作），我们将使用late_callback样式模式（在 BPF 之外执行这些工作，可能通过工作队列执行）。
lwn.net
例如，如果我们想进行更复杂的热计算，而这些计算不适合原子上下文，我们可以将其调度到延迟上下文中。挑战在于如何将大部分操作保留在 BPF 中以保持原子性和速度，我们相信这对于核心决策来说是可行的。
异构性和平台多样性：在一个平台上有效的策略可能需要在另一个平台上进行调整。Big.LITTLE 系统的行为与同构系统不同；同样，Intel x86 CPU 具有Turbo Boost 2.0/3.0、Speed Shift和 PC6 深度空闲状态等功能，这些功能部分由硬件/固件管理。在 Intel 上，如果我们在硬件控制模式下使用 intel_pstate，操作系统对频率的控制将受到限制——因此，为了全面测试 BEAR，我们可以将 intel_pstate 切换到被动模式或使用 acpi-cpufreq 驱动程序。这是为了通用性而设的限制：并非所有平台都允许细粒度的操作系统控制（Intel 的硬件可能会覆盖，而某些 ARM 系统通过 SCMI 将 DVFS 卸载到固件）。在这种情况下，BEAR 可能会受到限制或其优势有限。我们将记录这些场景，并在可能的情况下在具有完全操作系统控制的平台上进行测试。我们期望在操作系统控制的平台上（包括大多数 Android 设备、Raspberry Pi 等），BEAR 能够按预期运行。对于使用固件 DVFS 的设备，BEAR 仍然可以管理空闲和热管理，但频率设置最多只能提供建议。我们可能会考虑集成此类固件接口（例如，在可用的情况下发送提示）。确保我们的解决方案能够跨不同的内核和设备运行是一项挑战；为了缓解这一挑战，我们将专注于一些具有代表性的设备，并确保 BEAR 能够通过策略代码进行配置以处理它们的怪癖。eBPF 方法实际上在这方面有所帮助：人们可以加载针对特定平台优化的略有不同的 BPF 程序，而无需更改内核，这比一刀切的调控器具有巨大的优势。
lwn.net
。因此，从长远来看，供应商可以提供自己的 BEAR BPF 策略，例如，了解其特定的热传感器延迟或使用有效操作点的专有知识。
验证和主线化关注点：要说服 Linux 社区接受新的调控器方法，需要确凿的证据。一个可能的挑战是，人们对使用 BPF 来实现这一目的持怀疑态度（尽管考虑到最近的趋势，它可能受到欢迎，因为它不会影响那些未选择加入的用户）。我们将尽早与社区沟通，发布 RFC 补丁和性能测试结果。我们还应确保即使未加载 BEAR，开销也为零，内核运行起来就像没有更改一样（因此，我们会将所有钩子置于配置选项下，或默认使其处于惰性状态）。另一个担忧可能是安全性：eBPF 功能强大，存在缺陷或恶意的 BPF 程序理论上可能会对硬件进行错误管理（例如，禁用冷却功能导致损坏）。内核 BPF 验证器提供了内存安全性，但它无法阻止逻辑错误。在主线化中，他们可能要求只有特权用户才能加载此类调控器（这没问题——毕竟只有 root 才能加载此类 eBPF 程序）。我们将强调安全措施，并可能加入健全性检查（例如，在 BPF 中绝不允许频率超过最大值等）。这项挑战更多地关乎感知；我们评估的彻底性（尤其是显示无损害、仅带来好处）将是关键。
测量噪声和可重复性：在处理功耗和性能时，环境因素（环境温度、后台任务）可能会引入噪声。我们计划编写脚本并实现测试自动化，以最大程度地减少差异，并进行足够多的迭代以将其纳入考量。在热测试中，我们会尽可能控制环境温度或相应地校准结果。确保比较的公平性（基准测试和 BEAR 测试的条件相同）至关重要——我们会在冷却一段时间后连续使用相同的工作负载脚本进行测试。在某些情况下，我们可能会使用模拟或仿真进行受控比较（例如，使用英特尔的运行平均功率限制 (RAPL) 来强制执行恒定功耗场景以比较效率）。
lwn.net
）。
总而言之，尽管存在挑战，但我们的计划将通过精心设计、回退策略以及利用 BPF 的灵活性来应对这些挑战。分阶段的方法使我们能够逐步验证每个组件（首先是频率控制，然后是空闲状态，然后是热性能，最后是 GPU），而不是一次性解决所有问题。在整个过程中，我们将从先前的工作中学习——例如，如果我们的空闲状态预测失败，我们可以与 Rafael Wysocki 的 TEO 算法进行比较。
lwn.net
识别我们遗漏的内容；如果频率决策出现波动，我们可能会采用类似按需采样率的过滤
lwn.net
这些挑战是可以克服的，克服这些挑战将会产生一个强大的总体框架。
预期影响和未来方向
如果成功，BEAR 将代表操作系统电源管理领域的重大进步。对于学术研究而言，它提供了一个全新的平台，能够以前所未有的便捷性在真实操作系统中实现和评估电源管理策略。开发新型 DVFS 或调度算法的研究人员可以将其原型化为 BPF 程序，并在真实硬件上进行测试，而无需重建内核或说服内核维护人员接受实验性的调控器。这可以加速功耗感知计算领域的创新周期。例如，未来基于机器学习的电源控制（一个新兴领域）的研究
arxiv.org
arxiv.org
) 可以通过将训练好的模型作为 eBPF 策略插入来利用 BEAR。我们预计 BEAR 未来将能够加载针对特定应用优化的不同 eBPF 策略，从而实现“应用感知电源管理”。这可能与用户空间的编排器相结合，根据前台应用选择 BPF 程序（类似于 Android 针对不同场景使用提示的方式，但更加动态）。 从实用角度来看，BEAR 可以提升现有系统的能效和性能。移动设备在高负载情况下可以获得更长的电池续航时间和更稳定的性能（例如，减少因热节流导致的游戏或视频录制突然卡顿的情况）。数据中心可以部署 BEAR，在低负载时段降低功耗，同时快速响应负载峰值，从而节省能源成本并简化大规模热管理。一个有趣的可能性是使用 BEAR 在云环境中强制执行每个容器或虚拟机的功率或热量预算——因为 BPF 可以按 cgroup 过滤决策（通过一些扩展，我们可以提供按 cgroup 的频率建议）。这可以实现租户之间类似“功率 QoS”的隔离。 对于 Linux 内核社区来说，BEAR（或其组件）可能会影响未来调控器的设计方式。即使 BEAR 本身没有按原样合并，其经验教训（以及性能数据）也可能鼓励进一步集成 cpufreq 和 cpuidle。这可能会重新引发关于统一电源调度器的讨论，这一次，我们有证据表明它有效，并且灵活性问题可以通过 BPF 解决。我们还预计 cpufreq_ext 的工作将继续进行；我们的项目可以通过添加空闲和热上下文来补充它。BEAR 的某些部分有可能作为可选模块上传到上游——例如，bpf-cpufreq 调控器和 bpf-cpuidle 调控器，以及内核示例中的 BPF 示例代码。像 Rafael Wysocki（项目经理负责人）这样的维护者一直对新想法持开放态度，只要它们不会损害现有系统。
lwn.net
通过证明 BEAR 在多种场景下都能比默认调控器实现更高的效率，我们有力地证明了应该纳入这样一个框架。BPF 的使用也符合内核让子系统更可编程的方向（BPF 在网络、安全、IO 调度器等领域已经势头强劲，因此电源管理是合乎逻辑的下一步）。 此外，这些想法可以扩展到 CPU 和 GPU 之外：我们可以设想通过统一的策略协调内存频率（LPDDR DVFS）和其他组件（显示器刷新率等）与 CPU/GPU——现代智能手机正在尝试使用供应商解决方案。BEAR 可以扩展为全系统功耗调节器。另一个未来方向是将 BEAR 用于实时系统，通过创建保证截止时间同时最小化能耗的策略（例如，如果提前完成，则使用检查点到较慢的状态——这是实时 DVFS 研究中的一个概念）。BPF 方法具有足够的确定性，可用于实时场景（因为 BPF 代码执行时间是有限的），这可能对功耗感知实时调度研究具有吸引力。
es.cs.rptu.de
。 另一个影响领域是教育：拥有一个开放的、易于修改的电源调节器将成为教授操作系统电源管理或让新开发人员无所畏惧地进行实验的绝佳工具（因为如果经过验证，你不能轻易地用 BPF 使内核崩溃，而编写内核调节器模块很容易出错）。 总而言之，BEAR 旨在将两全其美的优势结合起来——协调电源管理策略的全局洞察力与 eBPF 支持的精细控制和敏捷性。它建立在丰富的研究和内核开发基础之上，解决了长期存在的电源策略碎片化问题。
lwn.net
androidauthority.com
利用现代可扩展的解决方案。通过验证 BEAR 在移动、服务器和嵌入式环境中的有效性，我们的研究不仅将展现其立竿见影的效益（例如，在受限条件下节省能源、提升性能），还将为生产系统中的自定义优化开辟新的途径。我们期望 BEAR 能够激发进一步的创新，例如，社区贡献的针对特定工作负载优化的 BPF 调控器库（例如，“AI 推理 BEAR 策略”或“游戏 BEAR 策略”），从而培育一个电源管理可以持续演进而无需等待内核发布周期的生态系统。在日益受到能耗和散热限制的计算世界中，电源管理的这种适应性和智能性不仅是可取的，更是不可或缺的。我们的提案将利用 eBPF 的强大功能来实现真正全面的 CPU 电源治理，​​朝着这一未来迈出了决定性的一步。 参考文献：（选定的内联引用） 
lwn.net
Corbet, J. “功耗感知调度的新方向。” LWN.net，2013年10月。（指出cpuidle、cpufreq和调度程序之间缺乏集成）
androidauthority.com
Sims, G. “ARM 为 Android 带来节能调度功能。” AndroidAuthority，2015 年 12 月。（电源子系统各自为政，且用途各异）
androidauthority.com
Sims, G. (2015). 通过 ARM IPA（热预算利用率）统一分配 CPU/GPU 功率的解释。
androidauthority.com
Sims, G. (2015).ARM IPA 结果：通过动态热预算，性能可提高高达 36%。
androidauthority.com
Sims, G. (2015). 能量感知调度 (EAS) 将 CPUFreq、CPUIdle 和 IPA 与能量模型集成在一起。
androidauthority.com
Sims, G. (2015).EAS 示例：将新任务放在已经活跃的核心上以节省能源，而不是唤醒空闲的核心。
arxiv.org
Karzhaubayeva, M. 等人，“剖析移动 DVFS 调速器对 LLM 推理的影响…” arXiv 2023。（先前的研究探索了最优频率组合，但没有研究 DVFS 调速器之间的相互作用）
lwn.net
Rybczyńska, M. “改善无滴答系统中的空闲行为。” LWN.net，2018 年 12 月。（CPUidle 菜单调控器的启发式算法和潜在的错误预测）
lwn.net
Rybczyńska, M. (2018).由于中断模式检测等原因导致菜单调节器准确性问题。
es.cs.rptu.de
（关于在调度中整合 DVFS 和 DPM 的研究论文——概述了组合方法的优势）。
arxiv.org
Hsieh, C.-Y. 等人，“面向移动游戏的内存感知协同 CPU-GPU DVFS 调速器。” IEEE ESTIMedia 2015。（集成内存、CPU、GPU DVFS 以应对游戏工作负载）
arxiv.org
Park, J.-G. 等人，“ML-Gov：一种用于移动游戏的机器学习增强型集成 CPU-GPU DVFS 调速器。” IEEE/ACM ESTIMedia 2017。（使用机器学习协调 CPU/GPU 频率）
arxiv.org
Park, J.-G. 等人 (2015)。“协同 CPU-GPU 频率上限 (Co-Cap) 实现节能移动游戏。” UCI 技术报告 2015。（在能耗上限下协调 CPU/GPU DVFS）
arxiv.org
Karzhaubayeva, M. 等人，“嵌入式系统上的 CNN 工作负载特性及集成 CPU-GPU DVFS 调速器”。IEEE ES Letters 15(4)，2023 年。（嵌入式 CNN 工作负载上的 CPU-GPU 集成 DVFS 策略）
arxiv.org
Kim, S. 等人，“zTT：面向移动设备的基于学习的零热节流 DVFS。” ACM MobiSys 2021。（通过主动频率控制避免热节流的 DVFS 策略）
arxiv.org
Sahin, O. 等人。“Maestro：热约束下移动应用的自主 QoS 管理。” IEEE TCAD 38(8)，2019 年。（移动设备上的热感知 QoS 和电源管理）
lwn.net
Zou, Y. [RFC 补丁] cpufreq_ext：基于 BPF 的 CPU 频率调节器。（动机：需要针对特定​​工作负载定制调节器，而非通用的内核调节器）
lwn.net
Zou, Y. (2024). （维护自定义调控器的内核模块面临挑战；BPF 提供了轻量级灵活的替代方案）
lwn.net
Zou, Y. (2024). （cpufreq_ext 与 sched_ext 协同工作，协调调度和频率调整决策，以实现最佳响应能力/功率）
phoronix.com
Larabel, M. “cpufreq_ext 正在开发基于 BPF 的 CPU 频率调节机制。” Phoronix，2024 年 9 月。（cpufreq_ext 概述：通过 BPF 实现可扩展的 CPU 频率调节，并与 sched_ext 集成）
free5gc.org
free5GC 博客。“Linux 中基于 BPF 的 CPU 调度器（sched_ext 概述）。” 2025 年 3 月。（优点：运行时动态策略变更、无需重启、通过 BPF 进行特定工作负载的优化）
研究.google
Humphries, JT 等人。“ghOSt：快速灵活的 Linux 调度用户空间委托”。SOSP 2021。（改进调度可以提高性能，但内核调度程序难以开发；在某些情况下需要定制策略）
研究.google
Humphries 等人（2021 年）。（ghOSt 解决方案：将调度委托给用户空间进程，允许任何语言和动态策略更改，且开销较低）
jauu.net
（Eco-Compute 2024 幻灯片）。（正在进行的研究：基于 eBPF 的 CPUIdle 调控器，用于动态、有针对性的空闲算法）
lwn.net
Zou, Y. (2024). (cpufreq_ext 定义了类似 get_next_freq 的 BPF 钩子，用于在 BPF 中进行自定义频率决策)
lwn.net
Zou, Y. (2024)。（未来工作：利用 BPF 根据系统范围的观测进行实时调整——这表明 BPF 管理器具有更多分析功能的潜力）
lwn.net
lwn.net
Corbet, J. (2013). （电源驱动方法：使用统一的 API 取代 cpuidle/cpufreq，并使用 go_faster/go_slower、enter_idle 等钩子，最终消除单独的子系统）
lwn.net
Corbet, J. (2013). （调度器通过 go_faster/go_slower 反馈，提示电源驱动器提高/降低性能，解耦精确频率控制）
lwn.net
Corbet, J. (2013). （原型中 go_faster/go_slower 的使用：当负载大于容量时，调度程序会调用 go_faster 来请求更多 CPU 性能等，包装现有的 cpufreq 驱动程序）
docs.kernel.org
Linux 内核文档。“CPU 空闲冷却”。（空闲注入是一种通过强制 CPU 空闲一段时间来冷却 CPU 的机制）
jauu.net
生态计算幻灯片（2024）。（启用 EAS 需要启用 Schedutil 和能源模型，指示调度程序和 cpufreq 的关联）
lwn.net
Linaro. “节能调度文档”。 (Linux 的 cpufreq 子系统与 CPUIdle 历来相互独立；EAS 将它们关联起来，形成 big.LITTLE 系统。)
lwn.net
Corbet, J. “OSPM 2025 峰会报告”。LWN.net，2025 年 5 月。（提到在工作状态下，组件由 cpufreq、cpuidle、powercap、thermal 等进行管理，在管理电源时需要考虑所有这些因素）
arxiv.org
Karzhaubayeva 等人（2023 年）。（许多 CPU/GPU 频率组合可以为 LLM 推理带来比默认调速器更佳的延迟和功耗——在相同功耗下延迟可降低高达 40%，凸显了默认 DVFS 的低效性。）
arxiv.org
Karzhaubayeva 等人（2023 年）。（发现：默认 GPU 调速器仅具备 GPU 能耗感知能力，无法与 CPU 调速器协调，导致结果不理想——强调了协调调速器的重要性。） （附加参考文献以【source†lines】格式嵌入上述内容。）






来源






你说：
有更多的现代处理器采用多种电源管理机制，尤其是动态电压和频率调节（DVFS）、空闲状态管理和热节流，以平衡性能和一致性。然而，这些传统上各自为政，通常无法享受到不一致的决策。在这三个方面以及湿度等方向优化的ebpf项目或者论文？如果只是机制ebpf方面的呢？



2/2

ChatGPT 说：

思考了 1 分 12 秒
以下是涉及 CPU 电源管理（DVFS、空闲、热）或纯能耗核算的公共 eBPF 项目和论文的快速浏览图。
我将它们分组，以便您可以看到哪些工作已经完成，哪些方面仍有差距。

1尝试直接控制DVFS/空闲/热的项目/论文
姓名和日期	它对 eBPF 的作用	子旋钮已覆盖	状态/备注
cpufreq_extRFC（Zou，2024）	将 cpufreq 调节器挂接到bpf_struct_ops表中（get_next_freq()等等），以便可以用 eBPF 而不是 C 编写策略。与 BPF 调度程序和 BPF DVFS 策略集成sched_ext以让 BPF 调度程序和 BPF DVFS 策略协同工作。	DVFS（每个策略频率）	linux-pm 和 bpf 列表上的 RFC。可在 6.9 及以上版本的内核上编译；包含粗略的示例策略。
lwn.net
eBPF CPU-Idle 调速器原型（Eco-Compute 峰会，2024 年）	用 BPF 钩子替换“菜单/TEO”cpuidle 调节器，以便可以在 eBPF 中决定空闲状态选择和空闲注入。	空闲状态（C 状态）、空闲喷射	学术原型；仅有幻灯片，但代码预计将由生态计算学生发布。
jauu.net
早期“动力驱动”和 BEAR 血统	Molnar/Rasmussen 于 2013 年提出的动力驱动理念是统一go_faster/go_slower/enter_idle。我们的 BEAR 概念只是通过 eBPF 对其进行了现代化改造。目前还没有公开代码，但它展示了内核社区正在讨论的方向。	DVFS + 空闲 + 热（目标）	设计理念；全面实施的机会（研究差距）。
jauu.net
lwn.net

实际情况：目前 cpufreq_ext 是唯一一个真正改变 CPU 频率的上游 eBPF 代码。空闲和热钩子仍处于研究原型阶段，因此如果你想发布，这个领域是开放的。

2 个专注于能源遥测/会计的eBPF 项目
（这些不会设置 DVFS 或空闲，但它们会提供驱动此类策略所需的每个进程或每个容器的能量数据。）

姓名	范围和技术	为什么重要
Linux 中的瓦特计/能耗感知进程调度(HotCarbon '24)	将 eBPF 程序附加到每个上下文切换，以便在内核中读取 RAPL MSR，从而以小于 1 µs 的开销实现毫秒级的每进程焦耳计算。用于在 ghOSt/sched_ext 之上构建能量公平和能量上限调度器。	提供准确、低开销的能量数字，可以为 DVFS/热策略提供支持。
asafcidon.com
开普勒（CNCF 沙盒，2023 年至今）	Kubernetes 的 Prometheus 导出器。使用 eBPF 跟踪点 + 性能计数器 + RAPL/NVML 将能耗归因于 Pod/容器；并为缺少 RAPL 的平台提供 ML 模型。	在云原生堆栈中迅速获得关注；集群级电源编排的良好数据源。
可持续计算.io
DEEP-mon（米兰理工大学，2018）	内核中的 eBPF 聚合调度程序事件以将权力分配给 Docker 容器；Phoronix 和 NPB 上的运行时开销小于 5%。	较旧但展示了内核聚合技巧，以避免 200 k sched-switch/sec 用户空间唤醒。
slideshare.net
eBPF-energy-monitor（GitHub 玩具项目）	最小 BCC 脚本，锁定sched_switch，跟踪每个 PID 的 CPU 时间，乘以来自 RAPL 的每核焦耳。	如果您只需要一个可运行的演示，这是一个方便的起点。
github.com
DEEP-mon 衍生品：BitWatts、Scaphandre	两者都提供软件定义的功率计；BitWatts 专注于虚拟机，Scaphandre 专注于裸机和 K8s。Scaphandre 可以选择编译 eBPF 传感器模块用于进程归因。	适合与开普勒比较准确性/开销权衡。
github.com
github.com

3 个快速外卖
目前很少有已发表的著作使用 eBPF 来控制DVFS/idle/thermal – cpufreq_ext 是主要的具体代码。

遥测技术已经成熟。开普勒、瓦特计和 DEEP-mon 已经提供了精细的焦耳计量数据，可供调速器用作反馈。

开放研究空间：将这些遥测源连接到基于 eBPF 的统一策略（BEAR 风格），该策略调用 cpufreq_ext + 未来的 cpuidle_bpf hook + 热帽，几乎仍未受到影响。

如果您需要任何特定项目的更多详细信息（代码指针、评估数字等），只需告诉我哪一个，我就会深入研究。












来源
你说：
下面是我在实验室中需要快速了解焦耳去向时使用的方法的“大全”，外加一个可以复制粘贴以绘制功率随时间变化图的简短 Python 脚本。 --- ## 1. 你到底想看什么？| 你关心的范围 | 典型接口 | 注释 | | ------------------------- | -------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------ | | **CPU 封装/DRAM** | Intel/AMD RAPL via /sys/class/powercap/intel-rapl*/energy_uj             | 显示为微焦耳中的“迄今为止使用的能量”。读取两次，将 ΔJ 除以 Δt 得到瓦特。([docs.kernel.org][1], [kernel.org][2]) | | **独立 NVIDIA GPU** | nvidia-smi --query-gpu=timestamp,power.draw -l 1                         | 提供瞬时板功率；大多数卡上的轮询间隔低至 50 毫秒。([nvidia.custhelp.com][3]) | | **整个服务器/笔记本电脑** | IPMI（ipmitool sdr list）、智能 PDU、USB-C PD 读数或智能插头 | 精度各不相同；适合用于测量墙上电源电压。| | **每个进程†** | powerapi、EnergyMeter或 eBPF + cgroup 标记 | 需要一些管道；非常适合分析应用程序。([github.com][4]) | （***†** 每个进程的数字始终是估计的；内核本身不会跟踪它。）--- ## 2. 一次性测量（CLI 风格）* **仅限 CPU，快速检查**

bash
# two samples, 1 s apart
E1=$(cat /sys/class/powercap/intel-rapl:0/energy_uj)
sleep 1
E2=$(cat /sys/class/powercap/intel-rapl:0/energy_uj)
echo "Power ≈ $(( (E2-E1)/1000000 )) W"


* **仅限 GPU**

bash
nvidia-smi --query-gpu=index,timestamp,power.draw --format=csv -l 1


* **一个仪表板中的所有内容** 1. 运行带有--collector.rapl标志的 **Prometheus node-exporter**。 2. 从 Prometheus 中抓取并添加 Grafana 面板。（[medium.com][5], [prometheus.io][6]）这将为您提供几乎零代码的实时图表。 --- ## 3. 使用 Python 进行连续记录 + 图表下面是一个最小脚本：1. 每秒对 RAPL 和 NVIDIA 进行采样，持续 \~60 秒。2. 计算瞬时功率。3. 绘制图表。

python
import time, pathlib, subprocess, csv
import matplotlib.pyplot as plt

rapl_path = pathlib.Path("/sys/class/powercap/intel-rapl:0/energy_uj")
def read_rapl():
    return int(rapl_path.read_text())

def read_gpu():
    out = subprocess.check_output(
        ["nvidia-smi", "--query-gpu=power.draw", "--format=csv,noheader,nounits"]
    )
    return float(out.decode().strip())

ts, cpu_pwr, gpu_pwr = [], [], []
E_prev = read_rapl()
t_prev = time.time()

for _ in range(60):             # ~1 minute
    time.sleep(1)
    E_now = read_rapl()
    t_now = time.time()

    cpu = (E_now - E_prev) / (t_now - t_prev) / 1e6   # W
    gpu = read_gpu()

    ts.append(t_now - ts[0] if ts else 0)
    cpu_pwr.append(cpu)
    gpu_pwr.append(gpu)

    E_prev, t_prev = E_now, t_now

plt.plot(ts, cpu_pwr, label="CPU (RAPL)")
plt.plot(ts, gpu_pwr, label="GPU (nvidia-smi)")
plt.xlabel("Time (s)")
plt.ylabel("Power (W)")
plt.legend()
plt.tight_layout()
plt.show()


提示： * 以非 root 身份运行此命令可能会在 RAPL 上失败；将自己添加到power组或使用 运行sudo。 * AMD CPU 改为公开amd_pmc计数器；路径不同。 * 在笔记本电脑上，您还可以阅读BAT0/power_now以查看放电功率。 --- ## 4. 更高级的工具 | 工具 | 为什么使用它 | 安装 | | ------------------------ | ------------------------------------------------------------------------ | ------------------------------------------------- | | **pyRAPL** | Python 基准测试的 CPU 能量（“使用 Measure(…) 作为 m: …”）| pip install pyRAPL（[pyrapl.readthedocs.io][7]）| | **powertop / powerstat** | 快速交互式调整 / CSV 日志记录 | 发行版包 | | **EnergyMeter** | 融合 pyRAPL + NVIDIA + eBPF 的 Python 包装器 | pip install energymeter（[github.com][4]）| | **Grafana Cloud / Loki** | 长期存储和漂亮的仪表板 | SaaS | --- ## 5. 常见问题 * 在某些 CPU 上，能量每隔几分钟就会环绕一次**——总是模数max_energy_range_uj。 * **空闲电源**在笔记本电脑上占主导地位；如果您只关心代码路径，请关闭涡轮增压。 * **轮询太快**会增加开销；1 Hz 对于趋势线来说已经足够，10-100 Hz 对于微基准测试来说已经足够。 * **电池支持的 UPS** 读数会滞后几秒钟——对于总消耗来说还可以，但对于细粒度的分析来说则不然。 --- 这就是要点。抓取与您的硬件匹配的任何接口，以固定的间隔对其进行采样，将值放入时间序列数据库或简单的 CSV 中，然后绘图。如果您遇到奇怪的边缘情况（包计数器冻结、GPU 重置等），请告知我们，我们可以深入挖掘。 [1]: https://docs.kernel.org/power/powercap/powercap.html?utm_source=chatgpt.com “功耗上限框架 - Linux 内核文档” [2]: https://www.kernel.org/doc/Documentation/power/powercap/powercap.txt?utm_source=chatgpt.com “powercap.txt - Linux 内核档案” [3]: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries?utm_source=chatgpt.com “有用的 nvidia-smi 查询 - nvidia 支持” [4]: https://github.com/maufadel/EnergyMeter?utm_source=chatgpt.com “maufadel/EnergyMeter：一个用于测量功耗的 Python 工具......” [5]: https://medium.com/%40dreams-smoke/how-to-monitor-power-usage-of-your-linux-computing-system-with-prometheus-grafana-f21b9933762?utm_source=chatgpt.com “如何使用...监控 Linux 计算系统的电源使用情况” [6]: https://prometheus.io/docs/guides/node-exporter/?utm_source=chatgpt.com “使用 Node Exporter - Prometheus 监控 Linux 主机指标” [7]: https://pyrapl.readthedocs.io/en/latest/?utm_source=chatgpt.com “欢迎使用 pyRAPL 文档！— pyRAPL 0.2.0..." ===为什么这些人使用 ebpf 监控而不是现有的工具，ebpf 能带来什么好处？


ChatGPT 说：

思考了37秒
监控功耗时要选择 eBPF，而不是直接使用现成工具？
现成方案	常见做法	袍
CLI 轮询 ( cat energy_uj, nvidia-smi, ipmitool)	用户态循环每 1-1000 ms 读一次寄存器/文件	间隙系统调用→上下文切换多；只能得到「整机/整包」级数据；想分析到进程就得把/proc/$pid/stat组合在一起，代码复杂，精度差
powertop / powerstat / 节点导出器	还是用户态轮询，只是预留生成工具	低频轮询时粒度粗，高频时消耗CPU；无法和调度器事件1:1扫描
GPU专用工具 ( nvidia-smi)	只看GPU，且最小50毫秒间隔	和CPU事件不同步；无法按PID/容器拆分

eBPF 能做的 5 件事
事件驱动，不用轮询
把代码插到内核跟踪点（如sched_switch），只有进程线程真正切换时才跑一段指令；省掉 24 × 7 的繁忙循环。瓦特表在每次调度点读取 RAPL 就能做到毫秒级计量，而不是用 1 kHz 轮询
热碳网
。

核内累计，用户态偶尔取一次
焦耳值放在BPF图里累加；Prometheus/Grafana 1秒抓一次即可。Kepler
的作者直说：把巨型PromQL下推到BPF后，Prometheus端查询开始明显恢复 
可持续计算.io
。

粒度任你定
因为在sched_switch里可以看到pid/ cgroup，DEEP-mon能把抽屉直接归到集装箱，平均托盘 < 6 % ，系统进货仍 < 5 %
researchgate.net
。同样的方法也能细化到线程、NUMA节点甚至eBPF映射键。

可热插拔、
即时eBPF程序bpftool prog load；想加GPU、内存频率、新ML模型不用改内核，也不怕ABI变化。Kepler就在运行时选择不同的ML功耗模型来加载没RAPL的Arm服务器
redhat.com
。

安全&低头
BPF验证器保证无越界、无死循环；指令级JIT后进内核L1服务器，平均一次执行只需几十纳秒。HotCarbon'24的瓦特计展示在每次上下文切换时增加的额外时间只有几微秒 
热碳网
。

eBPF什么时候特别划算？
要毫秒甚至μs级曲线：比如做键盘调度、DVFS实验。用户态很难做到1 kHz还不会烧毁CPU。

要分主体（进程 / cgroup / 容器）：内核里天然知道PID和cgroup ID，用map聚合最便捷。

集群级统一出口：Kepler 把 RAPL、NVML、ACPI 读取、perf 统计全部封装，暴露成 Prometheus 指标，K8s 里直接抓取就行。

想跟内核其他钩子就好：比如把“墨水/帧”写进 eBPF 调度器或 cpufreq_ext 频率调节器；不用再来回跨环 0/3。

总结
现有工具适合瞬时人肉看图；eBPF 更实时、更细粒度、低预装、可扩展的在线监控/闭环控制。
如果你只是偶尔想看整机功率，powertop已经够了；但要在调度器里做报表调度、或者给 BEAR 这种统一的调速器满足实时报警信号，eBPF 基本是唯一不拆内核又不压系统负载的方案。












来源
你说：
设置DVFS或者idle，或者在ebpf里面动态调整freq/energy能带来什么好处？真的有必要吗？


ChatGPT 说：

思考了4秒
为什么要在eBPF里“自己管”DVFS/idle？

典型诉求	传统习俗	eBPF 动态管控能力带来额外好处	什么时候“有需要”
降低电压 / 提电池	靠内核默认调节器（schedutil、menu/TEO）	结合调度事件、负载特征、温度实时最优化P/C-state；针对特定App可省5-30%电（已在Android定制调速器里见过）	移动设备、电池供电物联网；对远程敏感、负载模式单一（游戏、摄像）
稳定帧率 / 避免温度跳水	帕帕等热节流；温度超了再降频	提前预测热量，把频率慢慢收掉或注入空闲，平均FPS更稳定；可以把“突降”变成“缓降”	连续长时间满载（录4K、跑LLM）且不能掉帧
按机场/码头划分资源预算	只能全机统一 RAPL / PL1	eBPF在sched_switch里实时累能，把预算切给高优机场；结合cpufreq_ext只降其他机场频率	多机场云、边缘节点需要电力隔离
实验/研究新策略	修改内核重新启动	eBPF代码热插拔，5秒换套算法；并sched_ext共同做“联合调度+DVFS”实验快倍	学术/性能团队要A/B间隙试验
每个平台（big.LITTLE、CPU+GPU）	供应商 blob、用户态监控进程	eBPF可以直接读取GPU负载、温度图，然后降低CPU频率让热预算给GPU——无供应商驱动也能做	SoC 自己做系统集成、依赖组件 HAL

真的“需要”吗？一张简表判断
工作负载简单、对墨水不敏感→默认调节器足够，eBPF只是锦上添花。

对每瓦性能或温度拐点都有硬约束（手游、电池无人机、5U机柜卡着PDU）→自定策略往往能出10-30%的空间。

做系统研究/定制产品→ eBPF 是当下最省事、最安全的内核实验手段，比写 LKM / 改源省几个数量级的维护成本。

一句话：
“用不用eBPF管电源，看你在乎多少瓦、多少度，以及你改善损失的代价不能收回。”

如果只是想看个大概功率曲线，powertop就足够了；但要做细粒度、自适应、可热更新的功耗或温度控制，eBPF给出的“事件驱动+内核态聚合+安全热插拔”组合基本无可替代。








You said:
除了能和调度关联起来还可以和 ebpf 里面或者系统里面的什么事件关联起来


ChatGPT said:

Thought for 6 seconds
下面给你一份“事件菜单”——除了 sched_switch ，几乎所有能在内核里冒泡的信号都能成为 eBPF-DVFS/idle/热管理策略的触发点。我把它们按“能量直接相关”→“负载/行为侧信号”→“外部约束”三个层次列出来，顺便标注实际可用的 tracepoint/kprobe 名称，方便你马上试。

层次	代表事件 & 可挂 hook	用途举例
1. 电源/温度本身	power:power_cpu_frequency, power:cpu_idle, thermal:thermal_temperature, thermal:thermal_power_cap, devfreq:devfreq_trans	• 频率/idle/温度变化即时回调：
 → 进入高温区提前降频，而不是等节流
• GPU/内存 devfreq 事件驱动 CPU 让出功耗预算
2. 微架构性能信号	PMU overflow perf-events (cycles, mem-stall, LLC_miss, …)；perf_event_output map	• 当 mem_stall 计数溢出 → 判断是 memory-bound，降低 CPU P-state
• 当 instructions/cycle 低时注入 idle slice（race-to-idle）
3. 负载行为	① I/O block:block_rq_issue/complete, net:*, xdp prog
② 同步原语 futex:*, rwsem:*
③ IRQ/softirq irq:irq_handler_entry/exit, softirq_entry/exit	• 网络突发包量大 → 提前 boost 频率避免队列积压
• 块 I/O 过多 → 判断 I/O-bound，频率收益小，转而降低电压
• Futex 短同步风暴 → 先别进深 C-state，减唤醒抖动
4. 任务生命周期	sched:sched_process_fork/exec/exit, cgroup attach (cgroup_mkdir), LSM hook (security_bprm_check)
• 发现新高优 app fork → 对该 cgroup 提升 QoS 预算
• 低优 batch 进程退出 → 释放核心/热 budget 给前台
5. 周期/定时器	BPF timers (bpf_timer_*) 或 hrtimer_start kprobe	• 周期评估温度梯度 dT/dt → 自适应 PID 控制频率
• 长时滑窗重算 ML model 参数、写入 BPF map
6. 外部平台信号	ACPI “_PSR” 电源源变化 (acpi_psr trace)；USB-PD 电压/电流；Battery fuel-gauge (power_supply:power_supply_changed)	• 笔记本拔电→电池模式：触发省电策略
• PD 供电下降 → 提前降低峰值频率以免系统黑屏
7. 人为 QoS / 业务 KPI	用户态通过 BPF_MAP_UPDATE 写入指标（帧率、P99 延迟）	• 游戏帧率低于 55 FPS → 短时 boost Big cores
• Web 服务 P99>200 µs → 动态涨 CPU cap，再回收

要点

eBPF 可以把 多个事件流 汇总到同一份 map，再在任一 hook 里看到“全局状态”后一次性决策。（比如在温度 hook 里顺便查最近 10 ms 的网络包数，做 CPU⇆GPU 热 budget 交换）

事件驱动 ≠ 高频轮询：只有真正发生“值得关心的事”时才跑 BPF，功耗低、反应快。

同步/IRQ tracepoints 本身就带时间戳，可用来估算负载波形；再配合 PMU overflow 可以把 DVFS 调整做到 <1 ms 级延迟。

最小实践示例
在 power_cpu_frequency tracepoint上挂 BPF，实时记录每个 policy 的当前 P-state。

在 thermal_temperature 上再挂一个 BPF，发现温度逼近阈值时：

读取步骤 1 缓存的 P-state，看还有没有降频空间；

同时查 BPF map 里最近 5 ms 的 xdp 包数——若 GPU 负载重 & 网络 I/O 高，则只降 CPU，保持 GPU 频率；否则双降。

一旦决策，调用 bpf_cpufreq_set_next_freq() （来自 cpufreq_ext）调整所有 big cores 频率，并写入 map 供后续事件参考。

这样就把 功率、温度、I/O、网络 四条线合到了同一个闭环里，完全脱离用户态 busy loop。

结论

eBPF 能关联的不只是调度事件，而是整条内核“生命线”——性能计数器、温度、I/O、IRQ、电源状态甚至业务 KPI。
把多源事件融合在内核态做 先知式 DVFS/idle/thermal 决策，你就能在 µs-ms 级窗口内追着负载“跳舞”，而不是落后几百毫秒再救火。
