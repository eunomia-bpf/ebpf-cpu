# **引言**

现代处理器提供了丰富的可编程硬件机制，用于在性能与能耗间进行调优。例如，x86 平台支持基于离散电压/频率（P-态）的动态调节，以及多级空闲态（C-态）的深度节能[lenovopress.lenovo.com](https://lenovopress.lenovo.com/lp1946-using-processor-performance-p-states-with-linux-on-intel-based-servers#:~:text=Intel%20has%20implemented%20a%20series,constraints%2C%20which%20include%20the%20following)[lenovopress.lenovo.com](https://lenovopress.lenovo.com/lp1945-using-processor-idle-c-states-with-linux-on-thinksystem-servers#:~:text=Introduction)。同时，这些处理器还具有可编程的缓存控制、性能监控单元、热管理和资源隔离功能等。本文将系统调研主流 CPU（包括 x86、ARM、RISC-V 等）在硬件层面提供的可通过指令或内核接口编程控制的功能，分析其接口规范及可移植性。重点关注在 Linux 平台上利用 eBPF 对这些功能进行监控和调节的可行性，包括 eBPF 的能力边界、性能开销和安全性，并与传统方法（如内核模块、MSR 工具、perf 等）进行比较。

## **功能分类与技术细节**

### **P-态（性能态）与 DVFS**

P-态指处理器的离散性能状态，不同态对应不同的电压和频率配置。以 Intel 为例，增强型 SpeedStep 技术（EIST）定义了一组可选 P-态，操作系统可以通过写入 MSR（如 **`IA32_PERF_CTL`**）来切换到不同的 P-态[lenovopress.lenovo.com](https://lenovopress.lenovo.com/lp1946-using-processor-performance-p-states-with-linux-on-intel-based-servers#:~:text=Intel%20has%20implemented%20a%20series,constraints%2C%20which%20include%20the%20following)。处理器还通过 **`IA32_MPERF`** 和 **`IA32_APERF`** 硬件计数器反馈实际运行频率给 OS，以便根据利用率动态调整 P-态[lenovopress.lenovo.com](https://lenovopress.lenovo.com/lp1946-using-processor-performance-p-states-with-linux-on-intel-based-servers#:~:text=)。Linux 内核提供统一的 CPUFreq 子系统（通过 **`/sys/devices/system/cpu/cpufreq/`** 接口）管理这些硬件机制[lenovopress.lenovo.com](https://lenovopress.lenovo.com/lp1946-using-processor-performance-p-states-with-linux-on-intel-based-servers#:~:text=Processor%20P,and%20with%20the%20cpupower%20utility)，并可加载不同的调速 Governor（如 performance、ondemand、schedutil 等）来自动决策频率。Intel 平台通常使用 **`intel_pstate`** 驱动（结合 EIST MSR 或硬件自主 HWP 功能）或传统的 ACPI CPUFreq 驱动[lenovopress.lenovo.com](https://lenovopress.lenovo.com/lp1946-using-processor-performance-p-states-with-linux-on-intel-based-servers#:~:text=Processor%20P,and%20with%20the%20cpupower%20utility)；ARM 平台则通过 SoC 特定驱动和 PSCI 调用支持 DVFS；RISC-V 尚无统一的 DVFS 标准，通常依赖具体硬件的电源管理单元。通过这一接口，用户态工具（如 **`cpupower`**）可以显式设置频率范围和调速策略[lenovopress.lenovo.com](https://lenovopress.lenovo.com/lp1946-using-processor-performance-p-states-with-linux-on-intel-based-servers#:~:text=Processor%20P,and%20with%20the%20cpupower%20utility)。

### **C-态（空闲态）管理**

C-态表示 CPU 的空闲级别。x86 上，C1 怠眠通过执行 **`HLT`** 指令进入，保持缓存一致性；更深层次的 C2、C3 怠眠态则可由平台特定的指令（如 **`MWAIT`**）进入，并进一步关闭时钟或缓存域[lenovopress.lenovo.com](https://lenovopress.lenovo.com/lp1945-using-processor-idle-c-states-with-linux-on-thinksystem-servers#:~:text=,state%20does%20not)。ACPI 标准通过 **`_CST`** 表向 OS 报告可用的 C-态。Linux 内核通过 CPUIdle 子系统管理这些空闲态决策[lenovopress.lenovo.com](https://lenovopress.lenovo.com/lp1945-using-processor-idle-c-states-with-linux-on-thinksystem-servers#:~:text=Introduction)，根据系统负载和延迟要求选择合适的 C-态，调用相应指令进入低功耗模式[lenovopress.lenovo.com](https://lenovopress.lenovo.com/lp1945-using-processor-idle-c-states-with-linux-on-thinksystem-servers#:~:text=Introduction)[lenovopress.lenovo.com](https://lenovopress.lenovo.com/lp1945-using-processor-idle-c-states-with-linux-on-thinksystem-servers#:~:text=,state%20does%20not)。C-态控制接口通常通过内核配置（例如选择 Idle Governor）和 ACPI 驱动实现，不同架构上以 WFI（ARM）或 HLT/MWAIT（x86）等指令形式体现。

### **缓存控制与隔离**

硬件平台提供多种缓存控制指令和机制：例如 x86 可通过 **`CLFLUSH`**/**`CLFLUSHOPT`** 等指令手动清空缓存行，ARM 通过 D-cache 清除指令（如 **`DC CIVAC`**）等实现缓存无效化。在性能隔离方面，Intel 引入了资源定向技术（RDT），其中缓存分区（CAT）允许按位方式将 L2/L3 缓存划分给不同任务。CAT 可通过直接操作 MSR（如写入每核心的 CBM 寄存器）来配置，但需要细粒度保护时可使用 Linux 提供的 **`resctrl`** 接口，由内核帮助管理[github.com](https://github.com/intel/intel-cmt-cat/wiki#:~:text=The%20MSR%20interface%20is%20used,on%20a%20per%20core%20basis)。同理，ARM 的 MPAM（Memory Partitioning and Monitoring）机制也通过 **`resctrl`** 暴露给操作系统，用于设置内存带宽或缓存份额等参数[lwn.net](https://lwn.net/Articles/941673/#:~:text=Setup%20priority%20partitioning%20control%20under,and%20MPAM%20bandwidth%20partitioning)。在 Linux 中，**`/sys/fs/resctrl`** 的 **`schemata`** 文件既用于 Intel CAT/MBA，也用于 ARM MPAM 的控制域配置[github.com](https://github.com/intel/intel-cmt-cat/wiki#:~:text=The%20MSR%20interface%20is%20used,on%20a%20per%20core%20basis)[lwn.net](https://lwn.net/Articles/941673/#:~:text=Setup%20priority%20partitioning%20control%20under,and%20MPAM%20bandwidth%20partitioning)。此外，现代 CPU 还支持可编程预取器：以 Intel 为例，新一代架构提供了MSR **`0x1A4`** 来使能或禁用不同层次的数据预取器（L1/L2 预取器）[stackoverflow.com](https://stackoverflow.com/questions/784041/how-do-i-programmatically-disable-hardware-prefetching#:~:text=In%202014%20Intel%20published%20info,by%20bholanath%20%20%2066)[stackoverflow.com](https://stackoverflow.com/questions/784041/how-do-i-programmatically-disable-hardware-prefetching#:~:text=,of%20this%20MSR%20are%20reserved)。这一机制主要用于精确性能分析（如 Intel MLC 工具测量内存延迟时禁用预取器），可由具有特权的代码（内核态或 msr 驱动）进行修改[stackoverflow.com](https://stackoverflow.com/questions/784041/how-do-i-programmatically-disable-hardware-prefetching#:~:text=,of%20this%20MSR%20are%20reserved)。

### **性能监控单元（PMU）与计数器**

主流 CPU 都内置硬件性能监控单元（PMU），提供各种事件计数器（如周期数、指令数、缓存命中/未命中次数等）。例如 x86 平台通过 MSR（如 **`IA32_PERFEVTSELx`** 和 **`IA32_PMCx`**）配置和读取计数器，Linux 则通过 **`perf_event_open`** 系统调用统一访问这些事件。ARM 和 RISC-V 平台也有相应的 PMU 寄存器（ARM 的 PMCR, PMXEVTYPER 等，RISC-V 有 **`cycle`**、**`instret`** 等 CSR）。应用程序通常借助 Linux 的 perf 事件接口或性能分析工具（如 **`perf`**）来使用这些计数器。值得注意的是，eBPF 程序同样可以访问 perf 事件数据：Linux 提供了 BPF helper **`bpf_perf_event_read_value`**，允许在 eBPF 程序上下文中读取指定 PMU 计数器的当前值，并获取启用时间和运行时间等信息[docs.ebpf.io](https://docs.ebpf.io/linux/helper-function/bpf_perf_event_read_value/#:~:text=This%20helper%20function%20reads%20the,buf_size)。因此，eBPF 可用于在线读取和记录性能计数器，用于性能分析和自适应调度。

### **温度控制与节流**

现代 CPU 配备了片上温度传感器和热管理机制：在温度超限时，硬件会自动降低时钟频率或电压（热节流）以防过热。操作系统通过内核的 thermal framework（如 **`coretemp`** 驱动）周期性读取温度传感器值，并可结合 CPUFreq/CPUIdle 策略主动调节功耗。典型地，Linux 使用 ACPI 或架构特定的接口获取温度信息，将其反馈给用户空间（如 **`hwmon`**），并允许设置温度门限和风扇控制。综合来看，温度调节主要通过 P-态、C-态的限制或软件施加的功率上限来实现，而不是单条指令控制；因此，接口主要在驱动和 ACPI 层面实现（如 Intel RAPL 能报告并限制功耗）。

### **分支预测与预取器调节**

现代 CPU 中的分支预测器通常自动工作，没有公开的调整接口；但近期出于安全（缓解 Spectre 类攻击）目的，Intel 在 MSR **`IA32_SPEC_CTRL`** 中引入了 IBPB/IBRS/STIBP 等位来控制分支预测/缓存隔离。这些机制主要用于安全隔离，而非性能调优，操作系统可通过特权寄存器开启或屏蔽相应功能。相比之下，硬件预取器控制可编程性更高：例如前述 Intel MSR **`0x1A4`** 的各个位可使能/禁用 L1/L2 数据预取器[stackoverflow.com](https://stackoverflow.com/questions/784041/how-do-i-programmatically-disable-hardware-prefetching#:~:text=In%202014%20Intel%20published%20info,by%20bholanath%20%20%2066)[stackoverflow.com](https://stackoverflow.com/questions/784041/how-do-i-programmatically-disable-hardware-prefetching#:~:text=,of%20this%20MSR%20are%20reserved)，ARM 体系上也有类似的辅助控制寄存器（如 ACTLR 寄存器的预取控制位）。通过写入这些寄存器或使用内核驱动（如 **`msr`** 驱动）即可改变预取器行为。需要注意的是，这类操作通常需要最高特权（ring 0）才能执行，一般不具备跨架构的统一接口。

### **QoS 资源隔离（Intel RDT / ARM MPAM）**

面向多租户和实时性能保证需求，Intel RDT 和 ARM MPAM 提供了多维度的资源分区与监控能力。Intel RDT 包括 Cache Allocation Technology (CAT)、Memory Bandwidth Allocation (MBA) 等功能，可以通过 MSR 直接配置也可以通过 Linux **`resctrl`** 文件系统进行统一管理[github.com](https://github.com/intel/intel-cmt-cat/wiki#:~:text=The%20MSR%20interface%20is%20used,on%20a%20per%20core%20basis)。ARM MPAM 类似地支持缓存份额和内存带宽划分，Linux 中对其的支持也基于 **`resctrl`** 机制[lwn.net](https://lwn.net/Articles/941673/#:~:text=Setup%20priority%20partitioning%20control%20under,and%20MPAM%20bandwidth%20partitioning)。这些 QoS 控制接口允许将缓存容量或内存带宽等资源预留给指定任务组，以实现隔离和服务质量保障。不过，目前这些机制在不同架构间并不通用：Intel RDT 主要见于高端 x86 服务器平台，ARM MPAM 则出现在部分基于 ARMv8.2+ 的处理器中。Linux 提供统一的文件系统接口来管理这些资源（如 **`/sys/fs/resctrl`** 下的 schemata），但底层实现依赖于架构特定的硬件支持。

## **相关工作**

已有研究和工程工作对上述功能的控制方式有所涉及。例如，Intel 官方文档和第三方分析详细介绍了 RDT 的编程模型和用途[github.com](https://github.com/intel/intel-cmt-cat/wiki#:~:text=The%20MSR%20interface%20is%20used,on%20a%20per%20core%20basis)；嵌入式文档和厂商白皮书对 ACPI P/C 态及 DVFS 实现机制进行了说明[lenovopress.lenovo.com](https://lenovopress.lenovo.com/lp1946-using-processor-performance-p-states-with-linux-on-intel-based-servers#:~:text=Intel%20has%20implemented%20a%20series,constraints%2C%20which%20include%20the%20following)[lenovopress.lenovo.com](https://lenovopress.lenovo.com/lp1945-using-processor-idle-c-states-with-linux-on-thinksystem-servers#:~:text=Introduction)。社区内也出现了针对 eBPF 的探索：华为的 Yipeng Zou 等提出了一个基于 BPF 的 CPUFreq 扩展 governor (cpufreq_ext)，允许用户通过 BPF 程序自定义频率调节策略[lwn.net](https://lwn.net/Articles/991991/#:~:text=The%20cpufreq%20ext%20is%20a,cpufreq%20governor%20in%20BPF%20program)。在 Linux 交流活动中，也有涉及 BPF 与电源管理的讨论：比如 FOSDEM 2024 的一场报告中特别提到“基于 eBPF 的 CPUIdle governor”正在研究中[archive.fosdem.org](https://archive.fosdem.org/2024/schedule/event/fosdem-2024-2061-advanced-linux-power-management-evaluation-using-perf/#:~:text=In%20addition%2C%20a%20new%20power,introduced%20and%20can%20be%20discussed)；Eco-Compute 2024 的研讨会上同样列出了“eBPF 驱动的 CPUIdle Governor”作为正在进行的研究方向[jauu.net](https://jauu.net/talks/eco-compute-linux-power-analysis.pdf#:~:text=,EAS%29%20scheduling)。另外，Falco 社区的性能对比测试表明，对于高频事件跟踪，eBPF 相比传统内核模块具有较大开销[github.com](https://github.com/falcosecurity/libs/files/8334709/Falco.eBPF.Kernel.Module.Performance.pdf#:~:text=match%20at%20L150%20As%20can,drivers%2C%20we%20also%20looked%20at)，但通过过滤无关事件可以显著降低系统负载[github.com](https://github.com/falcosecurity/libs/files/8334709/Falco.eBPF.Kernel.Module.Performance.pdf#:~:text=As%20the%20data%20quite%20clearly,data%20shows%20an%20expected%20overhead)。这些工作表明，eBPF 在提供动态、灵活控制方面有潜力，但也面临性能和权限的限制。

## **eBPF 在监控与调节中的应用分析**

eBPF 技术允许在 Linux 内核中加载经验证的用户定义程序，无需编写内核模块即可扩展内核功能[ebpf.io](https://ebpf.io/what-is-ebpf/#:~:text=eBPF%20is%20a%20revolutionary%20technology,code%20or%20load%20kernel%20modules)。它可以附加在多种钩子上（如调度器事件、tracepoint、perf 事件、cgroup 等），从而实现对运行时信息的采集和决策。对于 CPU 微架构调优，eBPF 可用于**监控**层面：通过 perf 事件映射（**`BPF_MAP_TYPE_PERF_EVENT_ARRAY`**），eBPF 程序可利用 **`bpf_perf_event_read_value`** 等助手函数直接读取性能计数器值[docs.ebpf.io](https://docs.ebpf.io/linux/helper-function/bpf_perf_event_read_value/#:~:text=This%20helper%20function%20reads%20the,buf_size)。借助这些计数，程序可以实时分析热点指令、缓存命中率、分支误预测率等，并驱动调度策略或触发其他控制逻辑。

在**控制/调节**层面，eBPF 的能力受限于内核允许的操作范围。eBPF 程序本身无法直接执行诸如写 MSR、执行 **`MWAIT`** 等特权指令，也不能随意修改系统状态；它只能调用受限的 helper 函数并操作 BPF map。因此，当前需要通过配合内核已有的机制才能间接调节硬件：例如，利用 eBPF 的 struct_ops 功能可以实现可编程的 governor，如前述的 cpufreq_ext，通过 BPF 回调决定下一个目标频率[lwn.net](https://lwn.net/Articles/991991/#:~:text=The%20cpufreq%20ext%20is%20a,cpufreq%20governor%20in%20BPF%20program)；sched_ext 则允许自定义调度类与调度决策。可以设想，eBPF 程序在特定事件（如任务唤醒、硬件中断）时修改 cgroup QoS、改变 CPU 亲和性、或者动态调整 CPUFreq/CPUIdle 设置，来影响 P-态/C-态 使用。已有研究提到利用机器学习结合 eBPF 动态调整 CPUIdle 策略[jauu.net](https://jauu.net/talks/eco-compute-linux-power-analysis.pdf#:~:text=,EAS%29%20scheduling)，表明这一方向的可行性。

在**性能与开销**方面，eBPF 经过 JIT 编译后运行效率较高，但相比纯 C 模块还是有额外消耗，尤其在高频率调用时更为明显。Falco 性能测试报告指出，在监测频繁事件（如系统调用）时，eBPF 方案相较于内核模块明显更慢[github.com](https://github.com/falcosecurity/libs/files/8334709/Falco.eBPF.Kernel.Module.Performance.pdf#:~:text=match%20at%20L150%20As%20can,drivers%2C%20we%20also%20looked%20at)；不过如果仅针对需要的事件安装 BPF 钩子，避免跟踪无关活动，整体开销可大幅下降[github.com](https://github.com/falcosecurity/libs/files/8334709/Falco.eBPF.Kernel.Module.Performance.pdf#:~:text=As%20the%20data%20quite%20clearly,data%20shows%20an%20expected%20overhead)。与之相比，传统方法如使用 **`/dev/msr`** 直接读写寄存器，代价在于需要上下文切换且无灵活逻辑；perf 工具虽功能强大，却启动复杂且不易动态插入策略。eBPF 的**安全性模型**要求所有 BPF 程序通过严格验证并且在沙箱中运行[ebpf.io](https://ebpf.io/what-is-ebpf/#:~:text=eBPF%20is%20a%20revolutionary%20technology,code%20or%20load%20kernel%20modules)，可防止内核崩溃和不安全操作，这是内核模块无法比拟的优势。然而，eBPF 需要相应的能力（通常是 **`CAP_SYS_ADMIN`**）才能加载程序，且缺少直接硬件访问权限，限制了它对底层特性的控制能力。

总体来看，eBPF 为动态监控和策略制定提供了极大灵活性，可以结合内核调度、cgroup 等机制实现部分微架构功能的调优，比如基于实时性能数据自适应调整频率或空闲策略（如新兴的 BPF CPUFreq/CpuIdle governor）。其性能开销虽需权衡，但在需要可扩展的用户定义调节逻辑时具有独特价值。

## **未来方向与建议**

未来的研究可继续探索 eBPF 与硬件调优的深度结合。一方面，增强调用接口或 helper（如直接控制功率域、温度阈值等）可以扩大 eBPF 的应用场景；另一方面，可在 Linux 中标准化更多跨架构的接口，例如统一的功率管理 API 或 QoS 控制接口，以利于 eBPF 方案的可移植性。此外，可借鉴机器学习等方法，让 eBPF 驱动的策略在大规模数据基础上自适应优化，比如动态预测负载并调整 DVFS 策略。性能方面，可继续优化 BPF JIT 生成的代码或硬件加速（未来处理器可能内置对 eBPF 的支持），以减少延迟开销。最后，考虑到安全性和可靠性，在开放 eBPF 调控能力的同时，还需严格权限和验证机制，确保用户态策略无法导致系统不稳定。随着 eBPF 生态和硬件性能管理机制的不断发展，基于 eBPF 的监控和调节方案有望成为现代 Linux 系统进行细粒度动态优化的重要工具。

**参考文献：** 本文内容参考了处理器厂商文档、Linux 内核文档、社区博客和研究论文等资料[lenovopress.lenovo.com](https://lenovopress.lenovo.com/lp1946-using-processor-performance-p-states-with-linux-on-intel-based-servers#:~:text=Intel%20has%20implemented%20a%20series,constraints%2C%20which%20include%20the%20following)[lenovopress.lenovo.com](https://lenovopress.lenovo.com/lp1945-using-processor-idle-c-states-with-linux-on-thinksystem-servers#:~:text=Introduction)[github.com](https://github.com/intel/intel-cmt-cat/wiki#:~:text=The%20MSR%20interface%20is%20used,on%20a%20per%20core%20basis)[lwn.net](https://lwn.net/Articles/941673/#:~:text=Setup%20priority%20partitioning%20control%20under,and%20MPAM%20bandwidth%20partitioning)[stackoverflow.com](https://stackoverflow.com/questions/784041/how-do-i-programmatically-disable-hardware-prefetching#:~:text=In%202014%20Intel%20published%20info,by%20bholanath%20%20%2066)[docs.ebpf.io](https://docs.ebpf.io/linux/helper-function/bpf_perf_event_read_value/#:~:text=This%20helper%20function%20reads%20the,buf_size)[lwn.net](https://lwn.net/Articles/991991/#:~:text=The%20cpufreq%20ext%20is%20a,cpufreq%20governor%20in%20BPF%20program)[github.com](https://github.com/falcosecurity/libs/files/8334709/Falco.eBPF.Kernel.Module.Performance.pdf#:~:text=As%20the%20data%20quite%20clearly,data%20shows%20an%20expected%20overhead)[github.com](https://github.com/falcosecurity/libs/files/8334709/Falco.eBPF.Kernel.Module.Performance.pdf#:~:text=match%20at%20L150%20As%20can,drivers%2C%20we%20also%20looked%20at)[ebpf.io](https://ebpf.io/what-is-ebpf/#:~:text=eBPF%20is%20a%20revolutionary%20technology,code%20or%20load%20kernel%20modules)[jauu.net](https://jauu.net/talks/eco-compute-linux-power-analysis.pdf#:~:text=,EAS%29%20scheduling)[archive.fosdem.org](https://archive.fosdem.org/2024/schedule/event/fosdem-2024-2061-advanced-linux-power-management-evaluation-using-perf/#:~:text=In%20addition%2C%20a%20new%20power,introduced%20and%20can%20be%20discussed)。

**Citations**

[**Using Processor Performance P-States with Linux on Intel-based ThinkSystem Servers > Lenovo Press**https://lenovopress.lenovo.com/lp1946-using-processor-performance-p-states-with-linux-on-intel-based-servers](https://lenovopress.lenovo.com/lp1946-using-processor-performance-p-states-with-linux-on-intel-based-servers#:~:text=Intel%20has%20implemented%20a%20series,constraints%2C%20which%20include%20the%20following)
[**Using Processor Idle C-States with Linux on ThinkSystem Servers > Lenovo Press**https://lenovopress.lenovo.com/lp1945-using-processor-idle-c-states-with-linux-on-thinksystem-servers](https://lenovopress.lenovo.com/lp1945-using-processor-idle-c-states-with-linux-on-thinksystem-servers#:~:text=Introduction)
[**Using Processor Performance P-States with Linux on Intel-based ThinkSystem Servers > Lenovo Press**https://lenovopress.lenovo.com/lp1946-using-processor-performance-p-states-with-linux-on-intel-based-servers](https://lenovopress.lenovo.com/lp1946-using-processor-performance-p-states-with-linux-on-intel-based-servers#:~:text=)
[**Using Processor Performance P-States with Linux on Intel-based ThinkSystem Servers > Lenovo Press**https://lenovopress.lenovo.com/lp1946-using-processor-performance-p-states-with-linux-on-intel-based-servers](https://lenovopress.lenovo.com/lp1946-using-processor-performance-p-states-with-linux-on-intel-based-servers#:~:text=Processor%20P,and%20with%20the%20cpupower%20utility)
[**Using Processor Idle C-States with Linux on ThinkSystem Servers > Lenovo Press**https://lenovopress.lenovo.com/lp1945-using-processor-idle-c-states-with-linux-on-thinksystem-servers](https://lenovopress.lenovo.com/lp1945-using-processor-idle-c-states-with-linux-on-thinksystem-servers#:~:text=,state%20does%20not)
[**Home · intel/intel-cmt-cat Wiki · GitHub**https://github.com/intel/intel-cmt-cat/wiki](https://github.com/intel/intel-cmt-cat/wiki#:~:text=The%20MSR%20interface%20is%20used,on%20a%20per%20core%20basis)
[**ARM: MPAM: add support for priority partitioning control [LWN.net]**https://lwn.net/Articles/941673/](https://lwn.net/Articles/941673/#:~:text=Setup%20priority%20partitioning%20control%20under,and%20MPAM%20bandwidth%20partitioning)
[**linux - How do I programmatically disable hardware prefetching? - Stack Overflow**https://stackoverflow.com/questions/784041/how-do-i-programmatically-disable-hardware-prefetching](https://stackoverflow.com/questions/784041/how-do-i-programmatically-disable-hardware-prefetching#:~:text=In%202014%20Intel%20published%20info,by%20bholanath%20%20%2066)
[**linux - How do I programmatically disable hardware prefetching? - Stack Overflow**https://stackoverflow.com/questions/784041/how-do-i-programmatically-disable-hardware-prefetching](https://stackoverflow.com/questions/784041/how-do-i-programmatically-disable-hardware-prefetching#:~:text=,of%20this%20MSR%20are%20reserved)
[**Helper Function 'bpf_perf_event_read_value' - eBPF Docs**https://docs.ebpf.io/linux/helper-function/bpf_perf_event_read_value/](https://docs.ebpf.io/linux/helper-function/bpf_perf_event_read_value/#:~:text=This%20helper%20function%20reads%20the,buf_size)
[**cpufreq_ext: Introduce cpufreq ext governor [LWN.net]**https://lwn.net/Articles/991991/](https://lwn.net/Articles/991991/#:~:text=The%20cpufreq%20ext%20is%20a,cpufreq%20governor%20in%20BPF%20program)
[**FOSDEM 2024 - Advanced Linux Power Management Evaluation using Perf**https://archive.fosdem.org/2024/schedule/event/fosdem-2024-2061-advanced-linux-power-management-evaluation-using-perf/](https://archive.fosdem.org/2024/schedule/event/fosdem-2024-2061-advanced-linux-power-management-evaluation-using-perf/#:~:text=In%20addition%2C%20a%20new%20power,introduced%20and%20can%20be%20discussed)
[**eco-compute-linux-power-analysis**https://jauu.net/talks/eco-compute-linux-power-analysis.pdf](https://jauu.net/talks/eco-compute-linux-power-analysis.pdf#:~:text=,EAS%29%20scheduling)
[**Falco eBPF & Kernel Module Performance**https://github.com/falcosecurity/libs/files/8334709/Falco.eBPF.Kernel.Module.Performance.pdf](https://github.com/falcosecurity/libs/files/8334709/Falco.eBPF.Kernel.Module.Performance.pdf#:~:text=match%20at%20L150%20As%20can,drivers%2C%20we%20also%20looked%20at)
[**Falco eBPF & Kernel Module Performance**https://github.com/falcosecurity/libs/files/8334709/Falco.eBPF.Kernel.Module.Performance.pdf](https://github.com/falcosecurity/libs/files/8334709/Falco.eBPF.Kernel.Module.Performance.pdf#:~:text=As%20the%20data%20quite%20clearly,data%20shows%20an%20expected%20overhead)
[**What is eBPF? An Introduction and Deep Dive into the eBPF Technology**https://ebpf.io/what-is-ebpf/](https://ebpf.io/what-is-ebpf/#:~:text=eBPF%20is%20a%20revolutionary%20technology,code%20or%20load%20kernel%20modules)


